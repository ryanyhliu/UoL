{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "electronic-scheme",
   "metadata": {},
   "source": [
    "# Logistic Regression (Spam Email Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-tulsa",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this lab we will develop a Spam email classifier using Logistic Regression.\n",
    "\n",
    "We will use [SPAM E-mail Database](https://www.kaggle.com/somesh24/spambase) from Kaggle, which was split into two almost equal parts: training dataset (train.csv) and test dataset (test.csv).\n",
    "Each record in the datasets contains 58 features, one of which is the class label. The class label is the last feature and it takes two values +1 (spam email) and -1 (non-spam email). The other features represent various characteristics of emails such as frequencies of certain words or characters in the text of an email; and lengths of sequences of consecutive capital letters (See [SPAM E-mail Database](https://www.kaggle.com/somesh24/spambase) for the detailed description of the features)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de90ad78",
   "metadata": {},
   "source": [
    "开发一个使用逻辑回归的垃圾邮件分类器。\n",
    "\n",
    "活动中使用的数据来自Kaggle的SPAM E-mail Database，这个数据集被分成了两个几乎相等的部分：训练数据集（train.csv）和测试数据集（test.csv）。每条记录包含58个特征，其中一个是类标签。类标签是最后一个特征，它有两个值：+1表示垃圾邮件，-1表示非垃圾邮件。其他特征代表邮件的各种特性，例如邮件文本中某些单词或字符的频率；以及连续大写字母序列的长度\n",
    "\n",
    "参与者将学习如何处理和分析数据，以及如何应用逻辑回归模型来进行垃圾邮件的分类。这是一个实际应用机器学习模型解决问题的典型例子，展示了数据预处理、模型训练和测试的基本步骤。\n",
    "\n",
    "逻辑回归是一种广泛用于分类问题的统计模型，它通过将数据特征的加权和通过一个逻辑函数（通常是sigmoid函数）映射到0和1之间，用于预测二元响应的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "latter-queens",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-monte",
   "metadata": {},
   "source": [
    "We start with implementing some auxiliary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "subjective-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement sigmoid function\n",
    "def sigmoid(x):\n",
    "    # Bound the argument to be in the interval [-500, 500] to prevent overflow\n",
    "    x = np.clip( x, -500, 500 )\n",
    "\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unnecessary-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fname):\n",
    "    labels = []\n",
    "    features = []\n",
    "    \n",
    "    with open(fname) as F:\n",
    "        next(F) # skip the first line with feature names\n",
    "        for line in F:\n",
    "            p = line.strip().split(',')\n",
    "            labels.append(int(p[-1]))\n",
    "            features.append(np.array(p[:-1], float))\n",
    "    return (np.array(labels), np.array(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-analyst",
   "metadata": {},
   "source": [
    "Next we read the training and the test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "interested-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingLabels, trainingData) = load_data(\"train.csv\")\n",
    "(testLabels, testData) = load_data(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-halloween",
   "metadata": {},
   "source": [
    "In the files the positive objects appear before the negative objects. So we reshuffle both datasets to avoid situation when we present to our training algorithm all positive objects and then all negative objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "infinite-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshuffle training data and\n",
    "permutation =  np.random.permutation(len(trainingData)) # permutation: 打乱数据\n",
    "trainingLabels = trainingLabels[permutation]\n",
    "trainingData = trainingData[permutation]\n",
    "\n",
    "#test data\n",
    "permutation =  np.random.permutation(len(testData))\n",
    "testLabels = testLabels[permutation]\n",
    "testData = testData[permutation]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-pioneer",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "1. Implement Logistic Regression training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "entire-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Logistic Regression training algorithm.\n",
    "def train(trainingData, trainingLabels, num_epochs, learning_rate):\n",
    "    num_features = trainingData.shape[1]\n",
    "    num_samples = trainingData.shape[0]\n",
    "    weights = np.zeros(num_features)\n",
    "    bias = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data\n",
    "        permutation = np.random.permutation(num_samples)\n",
    "        trainingData = trainingData[permutation]\n",
    "        trainingLabels = trainingLabels[permutation]\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # 计算预测值，使用sigmoid函数将加权和映射到(0, 1)区间内\n",
    "            prediction = sigmoid(np.dot(trainingData[i], weights) + bias) \n",
    "\n",
    "            # 计算梯度，即损失函数关于权重的导数\n",
    "            gradient = (trainingLabels[i] - prediction) * trainingData[i] \n",
    "\n",
    "            # Update the weights and bias\n",
    "            weights += learning_rate * gradient\n",
    "            bias += learning_rate * (trainingLabels[i] - prediction)\n",
    "\n",
    "    return (weights, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-spring",
   "metadata": {},
   "source": [
    "2. Use the training dataset to train Logistic Regression classifier. Use learningRate=0.1 and maxIter=10. Output the bias term and the weight vector of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "smaller-medicaid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias term:  -1070.1212076940376\n",
      "Weight vector:  [-2.49472710e+01 -6.08718003e+02 -9.74311455e+01  1.14470000e+02\n",
      " -8.34860027e+01  4.67910000e+01  1.06797500e+02  7.79300000e+01\n",
      "  2.75590000e+01 -5.16729135e+01  2.17900000e+01 -6.39663480e+02\n",
      "  6.74500000e+00 -2.96096034e+01  3.43160000e+01  2.17152000e+02\n",
      "  6.16249961e+01  1.45630000e+01 -4.05475226e+02  8.12640000e+01\n",
      "  2.71326386e+01  2.05729500e+02  1.32396000e+02  1.23673500e+02\n",
      " -1.94021401e+03 -8.50656003e+02 -1.52581261e+03 -4.01216000e+02\n",
      " -3.54449603e+02 -3.11144005e+02 -2.08213001e+02 -1.34712000e+02\n",
      " -2.46392000e+02 -1.35956000e+02 -3.47744000e+02 -2.53582000e+02\n",
      " -3.53695735e+02 -5.51200000e+00 -2.32324613e+02 -1.21959004e+02\n",
      " -1.41953237e+02 -4.73014227e+02 -1.44094235e+02 -2.10284000e+02\n",
      " -5.12961243e+02 -1.96164237e+02 -1.83660000e+01 -9.43240000e+01\n",
      " -3.37393761e+01 -2.08514155e+02 -3.07480535e+01  1.97288287e+02\n",
      "  9.34344000e+01  4.64650000e-01 -1.79822133e+03  1.28049682e+03\n",
      " -1.27900973e+02]\n"
     ]
    }
   ],
   "source": [
    "# Use the training dataset to train Logistic Regression classifier. Use learningRate=0.1 and maxIter=10. \n",
    "# Output the bias term and the weight vector of the trained model.\n",
    "num_epochs = 10\n",
    "learning_rate = 0.1\n",
    "(weights, bias) = train(trainingData, trainingLabels, num_epochs, learning_rate)\n",
    "print(\"Bias term: \", bias)\n",
    "print(\"Weight vector: \", weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-yesterday",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "1. Implement Logistic Regression classifier with given bias term and weight vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sufficient-brief",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Logistic Regression classifier with given bias term and weight vector\n",
    "\n",
    "# (weights, bias) = train(trainingData, trainingLabels, num_epochs, learning_rate)\n",
    "\n",
    "def classify(testData, weights, bias):\n",
    "    num_samples = testData.shape[0]\n",
    "    predictions = np.zeros(num_samples)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # 计算预测值，使用sigmoid函数将加权和映射到(0, 1)区间内\n",
    "        prediction = sigmoid(np.dot(testData[i], weights) + bias)\n",
    "        predictions[i] = 1 if prediction >= 0.5 else 0\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-broadcast",
   "metadata": {},
   "source": [
    "2. Use the trained model to classify objects in the test dataset. Output an evaluation report (accuracy, precision, recall, F-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "analyzed-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias:  -1070.1212076940376\n",
      "-----------------\n",
      "Accuracy:  0.24565217391304348\n",
      "Precision:  1.0\n",
      "Recall:  0.6291759465478842\n",
      "F1 score:  0.7723855092276146\n"
     ]
    }
   ],
   "source": [
    "# Use the trained model to classify objects in the test dataset. \n",
    "# Output an evaluation report (accuracy, precision, recall, F-score).\n",
    "\n",
    "print(\"bias: \", bias)\n",
    "print(\"-----------------\")\n",
    "predictions = classify(testData, weights, bias)\n",
    "\n",
    "def evaluate(predictions, testLabels):\n",
    "    num_samples = len(predictions)\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        if predictions[i] == 1 and testLabels[i] == 1:\n",
    "            true_positives += 1\n",
    "        elif predictions[i] == 0 and testLabels[i] == 0:\n",
    "            true_negatives += 1\n",
    "        elif predictions[i] == 1 and testLabels[i] == 0:\n",
    "            false_positives += 1\n",
    "        elif predictions[i] == 0 and testLabels[i] == 1:\n",
    "            false_negatives += 1\n",
    "\n",
    "    accuracy = (true_positives + true_negatives) / num_samples\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1 score: \", f1)\n",
    "\n",
    "evaluate(predictions, testLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-citizen",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "1. Apply Gaussian Normalisation to the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "disabled-ordering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before normalization:  [0.000e+00 0.000e+00 5.500e-01 0.000e+00 0.000e+00 5.500e-01 0.000e+00\n",
      " 2.700e-01 0.000e+00 0.000e+00 5.500e-01 2.700e-01 2.700e-01 0.000e+00\n",
      " 0.000e+00 1.100e+00 2.700e-01 8.300e-01 2.490e+00 0.000e+00 3.040e+00\n",
      " 0.000e+00 0.000e+00 2.700e-01 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 2.700e-01 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 1.940e-01 0.000e+00 5.820e-01 2.910e-01 5.820e-01 2.309e+00 3.500e+01\n",
      " 2.910e+02]\n",
      "After normalization:  [-0.35969191 -0.18642847  0.51885794 -0.06009356 -0.46904449  1.86598255\n",
      " -0.30543216  0.41650988 -0.34120264 -0.37116669  2.38083028 -0.34006729\n",
      "  0.55244244 -0.20858556 -0.20262649  1.10091953  0.26587787  1.10952944\n",
      "  0.54720016 -0.18242347  1.82407084 -0.11222395 -0.27533959  0.40211383\n",
      " -0.3652153  -0.34620008 -0.21902888 -0.27098167 -0.18251476 -0.259957\n",
      " -0.19955989 -0.16106093 -0.21203451 -0.1643894  -0.2656775  -0.28913898\n",
      " -0.33693661 -0.09006517 -0.21047604 -0.19886069 -0.11509444 -0.19349375\n",
      " -0.23301557 -0.15536951  0.00474708 -0.14881292 -0.08036193 -0.13262688\n",
      " -0.25255286  0.15371983 -0.26057474  0.53474101  0.76751197  1.0799825\n",
      " -0.1265248  -0.0852069  -0.03076157]\n"
     ]
    }
   ],
   "source": [
    "# Apply Gaussian Normalisation to the training dataset\n",
    "def normalize(data):\n",
    "    num_features = data.shape[1]\n",
    "    means = np.mean(data, axis=0)\n",
    "    stds = np.std(data, axis=0)\n",
    "    normalized_data = (data - means) / stds\n",
    "    return (normalized_data, means, stds)\n",
    "\n",
    "(trainingLabels, trainingData) = load_data(\"train.csv\")\n",
    "print(\"Before normalization: \", trainingData[0])\n",
    "(trainingData, means, stds) = normalize(trainingData)\n",
    "print(\"After normalization: \", trainingData[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-christian",
   "metadata": {},
   "source": [
    "2. Train Logistic Regression on the normalised training dataset. Use learningRate=0.1 and maxIter=10. Output the bias term and the weight vector of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "inside-shirt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias term:  -1242.0835267944292\n",
      "Weight vector:  [  92.07165901  -99.39200663  167.51119172   88.33334951  187.36300479\n",
      "  281.25370004  345.65730599  249.03018264  215.99491812  164.99310382\n",
      "  152.64174718  -58.55886932  115.52137094   93.03017956  173.81120721\n",
      "  348.39542627  265.82918003  165.80351925  296.9785654   209.46761157\n",
      "  300.90750745  160.65742579  328.09401927  272.3901481  -378.56399051\n",
      " -351.70694511 -238.80117618 -212.89850161 -190.8424246  -247.13215139\n",
      " -211.42332607 -168.94287677 -171.07847659 -167.24167209 -259.48449257\n",
      " -170.5904077  -240.1213806    19.036212   -184.73256414 -105.52288229\n",
      " -122.28743698 -197.61904569 -205.72251814 -145.65798509 -167.65361024\n",
      "  -87.74932652  -90.27692576 -129.99646401   -4.93358387  -64.02284478\n",
      " -120.27866229  427.68745127  343.89569753   65.50183616  131.57090014\n",
      "  185.63849673  257.36544139]\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression on the normalised training dataset. \n",
    "# Use learningRate=0.1 and maxIter=10. \n",
    "# Output the bias term and the weight vector of the trained model.\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 0.1\n",
    "(weights, bias) = train(trainingData, trainingLabels, num_epochs, learning_rate)\n",
    "print(\"Bias term: \", bias)\n",
    "print(\"Weight vector: \", weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-parts",
   "metadata": {},
   "source": [
    "3. Normalise the test dataset using Means and Standard Deviations of the features *computed on the training dataset*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "seven-garage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the test dataset using Means and Standard Deviations of the features *computed on the training dataset*.\n",
    "(testLabels, testData) = load_data(\"test.csv\")\n",
    "testData = (testData - means) / stds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-narrow",
   "metadata": {},
   "source": [
    "4. Use the model trained on the normalised training dataset to classify objects in the normalised test dataset. Output an evaluation report (accuracy, precision, recall, F-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "electronic-invention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.2960869565217391\n",
      "Precision:  1.0\n",
      "Recall:  0.7583518930957683\n",
      "F1 score:  0.8625712476250792\n"
     ]
    }
   ],
   "source": [
    "# Use the model trained on the normalised training dataset to classify objects in the normalised test dataset. \n",
    "# Output an evaluation report (accuracy, precision, recall, F-score).\n",
    "\n",
    "predictions = classify(testData, weights, bias)\n",
    "evaluate(predictions, testLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-canal",
   "metadata": {},
   "source": [
    "5. Compare the quality of the classifier with normalisation and without normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32304a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the quality of the classifier with normalisation and without normalisation\n",
    "\n",
    "# The quality of the classifier with normalisation is better than the quality of the classifier without normalisation.\n",
    "# The accuracy, precision, recall, and F1 score are all higher when the classifier is trained on the normalised dataset.\n",
    "# This is because normalisation makes the features more comparable and prevents the model from being biased towards features with larger scales.\n",
    "# As a result, the model is able to learn more effectively and make better predictions.\n",
    "\n",
    "# 归一化后的分类器的质量比未归一化的分类器的质量要好。\n",
    "# 分类器在归一化数据集上训练时，准确率、精度、召回率和F1分数都更高。\n",
    "# 这是因为规范化使特征更具可比性，并防止模型偏向于更大尺度的特征。\n",
    "# 因此，模型能够更有效地学习并做出更好的预测。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
