{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, inputs, outputs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=inputs, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=64)\n",
    "        self.fc4 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.out = nn.Linear(in_features=32, out_features=outputs)\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = F.relu(self.fc3(t))\n",
    "        t = F.relu(self.fc4(t))\n",
    "        t = self.out(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "    \"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    ")\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append(Experience(state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, device):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = (\n",
    "            torch.from_numpy(np.vstack([e.state for e in experiences if e is not None]))\n",
    "            .float()\n",
    "            .to(device)\n",
    "        )\n",
    "        actions = (\n",
    "            torch.from_numpy(\n",
    "                np.vstack([e.action for e in experiences if e is not None])\n",
    "            )\n",
    "            .long()\n",
    "            .to(device)\n",
    "        )\n",
    "        rewards = (\n",
    "            torch.from_numpy(\n",
    "                np.vstack([e.reward for e in experiences if e is not None])\n",
    "            )\n",
    "            .float()\n",
    "            .to(device)\n",
    "        )\n",
    "        next_states = (\n",
    "            torch.from_numpy(\n",
    "                np.vstack([e.next_state for e in experiences if e is not None])\n",
    "            )\n",
    "            .float()\n",
    "            .to(device)\n",
    "        )\n",
    "        dones = (\n",
    "            torch.from_numpy(\n",
    "                np.vstack([e.done for e in experiences if e is not None]).astype(\n",
    "                    np.uint8\n",
    "                )\n",
    "            )\n",
    "            .float()\n",
    "            .to(device)\n",
    "        )\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_vector_length,\n",
    "        num_actions,\n",
    "        alpha=0.001,\n",
    "        eps=1,\n",
    "        eps_decay=0.995,\n",
    "        eps_min=0.05,\n",
    "        gamma=0.9,\n",
    "        batch_size=64,\n",
    "        seed=42,\n",
    "    ):\n",
    "        self.num_actions = num_actions\n",
    "        self.eps = eps\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps_min = eps_min\n",
    "        self.gamma = gamma\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.step = 0\n",
    "        self.policy_net = DQN(state_vector_length, num_actions).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(params=self.policy_net.parameters(), lr=alpha)\n",
    "\n",
    "        self.memory = ReplayMemory(100000, batch_size, seed)\n",
    "\n",
    "        if seed != None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "    def select_action(self, s):\n",
    "        self.step += 1\n",
    "        if np.random.random() < self.eps:\n",
    "            action = np.random.randint(0, self.num_actions)\n",
    "        else:\n",
    "            action = self._get_best_action(s)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def _get_best_action(self, s):\n",
    "        with torch.no_grad():\n",
    "            action = (\n",
    "                self.policy_net(torch.tensor([s]).to(self.device))\n",
    "                .argmax(dim=1)\n",
    "                .to(self.device)\n",
    "                .item()\n",
    "            )\n",
    "        return action\n",
    "\n",
    "    def update_q(self, s, a, s_prime, r, done):\n",
    "        self.memory.push(s, a, r, s_prime, done)\n",
    "        self.step += 1\n",
    "\n",
    "        if done:\n",
    "            self.eps = max(self.eps_min, self.eps * self.eps_decay)\n",
    "\n",
    "        if len(self.memory) > self.memory.batch_size:\n",
    "            experiences = self.memory.sample(self.device)\n",
    "            self.learn(experiences)\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        next_q_values = self.policy_net(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        q_targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "        current_q_values = self.policy_net(states).gather(1, actions)\n",
    "\n",
    "        loss = F.mse_loss(current_q_values, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def save_network(self, outfile):\n",
    "        torch.save(self.policy_net.state_dict(), outfile)\n",
    "\n",
    "    def load_network(self, infile):\n",
    "        self.policy_net.load_state_dict(torch.load(infile))\n",
    "        self.policy_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lander_runner(\n",
    "    num_episodes,\n",
    "    target_update,\n",
    "    alpha,\n",
    "    eps,\n",
    "    eps_decay,\n",
    "    gamma,\n",
    "    seed,\n",
    "    convergence_threshold=200,\n",
    "    render=False,\n",
    "):\n",
    "    env = gym.make(\"LunarLander-v2\")\n",
    "    # env.seed(SEED)\n",
    "    agent = SimpleDQNAgent(\n",
    "        env.observation_space.shape[0],\n",
    "        env.action_space.n,\n",
    "        alpha=alpha,\n",
    "        eps=eps,\n",
    "        eps_decay=eps_decay,\n",
    "        gamma=gamma,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    for e in range(num_episodes):\n",
    "        cur_observation = env.reset()\n",
    "        if render:\n",
    "            env.render()\n",
    "        episode_reward = 0\n",
    "        for t in count():\n",
    "            action = agent.select_action(cur_observation)\n",
    "            next_observation, reward, done, truncated, info = env.step(action)\n",
    "            agent.update_q(cur_observation, action, next_observation, reward, done)\n",
    "            cur_observation = next_observation\n",
    "            episode_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                rewards.append(episode_reward)\n",
    "                # plot_rewards(rewards)\n",
    "                plt.pause(0.01)\n",
    "                print(f\"Episode {e}: {episode_reward}\")\n",
    "                # if is_ipython:\n",
    "                #     display.clear_output(wait=True)\n",
    "                break\n",
    "        # if e % target_update == 0:\n",
    "        #     agent.update_target()\n",
    "        # if np.all(moving_average(rewards, 100)[-100:] >= convergence_threshold):\n",
    "        #     print(f\"Solved in {e} episodes.\")\n",
    "        #     agent.save_network(f\"out\\\\agent.pt\")\n",
    "        #     break\n",
    "\n",
    "    env.close()\n",
    "    return rewards, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m run_rewards, agent \u001b[38;5;241m=\u001b[39m \u001b[43mlander_runner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_update\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0005\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.999\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m57\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvergence_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m210\u001b[39;49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 34\u001b[0m, in \u001b[0;36mlander_runner\u001b[1;34m(num_episodes, target_update, alpha, eps, eps_decay, gamma, seed, convergence_threshold, render)\u001b[0m\n\u001b[0;32m     32\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(cur_observation)\n\u001b[0;32m     33\u001b[0m next_observation, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m---> 34\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_observation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_observation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m cur_observation \u001b[38;5;241m=\u001b[39m next_observation\n\u001b[0;32m     36\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[18], line 57\u001b[0m, in \u001b[0;36mSimpleDQNAgent.update_q\u001b[1;34m(self, s, a, s_prime, r, done)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps_min, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps_decay)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[1;32m---> 57\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearn(experiences)\n",
      "Cell \u001b[1;32mIn[17], line 20\u001b[0m, in \u001b[0;36mReplayMemory.sample\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, device):\n\u001b[0;32m     17\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m     19\u001b[0m     states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 20\u001b[0m         torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     23\u001b[0m     )\n\u001b[0;32m     24\u001b[0m     actions \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     25\u001b[0m         torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\n\u001b[0;32m     26\u001b[0m             np\u001b[38;5;241m.\u001b[39mvstack([e\u001b[38;5;241m.\u001b[39maction \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m experiences \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     30\u001b[0m     )\n\u001b[0;32m     31\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     32\u001b[0m         torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\n\u001b[0;32m     33\u001b[0m             np\u001b[38;5;241m.\u001b[39mvstack([e\u001b[38;5;241m.\u001b[39mreward \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m experiences \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     37\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\isrya\\.conda\\envs\\LUNARLANDER\\Lib\\site-packages\\numpy\\core\\shape_base.py:286\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_vhstack_dispatcher)\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvstack\u001b[39m(tup, \u001b[38;5;241m*\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame_kind\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    221\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m    Stack arrays in sequence vertically (row wise).\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    284\u001b[0m \n\u001b[0;32m    285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 286\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m \u001b[43matleast_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    288\u001b[0m         arrs \u001b[38;5;241m=\u001b[39m [arrs]\n",
      "File \u001b[1;32mc:\\Users\\isrya\\.conda\\envs\\LUNARLANDER\\Lib\\site-packages\\numpy\\core\\shape_base.py:121\u001b[0m, in \u001b[0;36matleast_2d\u001b[1;34m(*arys)\u001b[0m\n\u001b[0;32m    119\u001b[0m res \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ary \u001b[38;5;129;01min\u001b[39;00m arys:\n\u001b[1;32m--> 121\u001b[0m     ary \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ary\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    123\u001b[0m         result \u001b[38;5;241m=\u001b[39m ary\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "run_rewards, agent = lander_runner(\n",
    "    num_episodes=1500,\n",
    "    target_update=4,\n",
    "    alpha=0.0005,\n",
    "    eps=1,\n",
    "    eps_decay=0.99,\n",
    "    gamma=0.999,\n",
    "    seed=57,\n",
    "    convergence_threshold=210\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LUNARLANDER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
