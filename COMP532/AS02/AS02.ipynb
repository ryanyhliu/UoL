{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前期准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# 创建Lunar Lander环境，这里使用离散动作空间的版本\n",
    "env = gym.make(\"LunarLander-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索环境\n",
    "了解 动作空间, 观察空间\n",
    "\n",
    "后续 代理设计 需要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(4)\n",
      "Observation Space: Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
      " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
      " 1.       ], (8,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space:\", env.action_space)\n",
    "print(\"Observation Space:\", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例代码\n",
    "\n",
    "随机动作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    \n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # print(\"{}, {}, {}, {}\".format(observation, reward, terminated, truncated))\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 显卡测试\n",
    "\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "pip install torch==2.2.2+cu118 torchvision==0.14.2+cu118 torchaudio==0.12.2+cu118 -f https://download.pytorch.org/whl/torch_stable.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:  2.2.2+cu118\n",
      "CUDA version:  11.8\n",
      "CUDA available:  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version: \", torch.__version__)\n",
    "print(\"CUDA version: \", torch.version.cuda)\n",
    "\n",
    "# 输出是否可以使用 CUDA\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正式工作 (使用DQN)\n",
    "\n",
    "训练一个Agent, 根据Observation和info, 决定最佳Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 导入并初始化环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 定义DQN网络模型\n",
    "\n",
    "通过PyTorch的nn模块, 来构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"state_size: 状态空间的维度\"\n",
    "        \"action_size: 动作空间的维度\"\n",
    "        super(DQN, self).__init__()\n",
    "        # 定义神经网络的结构\n",
    "        # 输入层的维度是状态空间的维度, 输出层的维度是动作空间的维度\n",
    "        # 输入层 -(fc1)> 128 -(fc2)> 64 -(fc3)> 输出层 (常见的三层全连接神经网络)\n",
    "        # fc1, fc2, fc3 分别是三个全连接层, 用于处理输入数据\n",
    "        self.fc1 = nn.Linear(state_size, 128) \n",
    "        self.relu = nn.ReLU() # relu 是激活函数, 用于增加网络的非线性 (可以在每个全连接层后面添加, 是一个超参数)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    # 前向传播: 计算出网络的输出和损失, 用来更新网络\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x) # 返回一个向量, 其维度 = 动作空间的维度. i.e., 每个动作的Q值, 即给定状态下动作的价值(分数 / 预期回报)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 实现DQN Agent\n",
    "\n",
    "创建一个Agent类, 用来实现DQN的训练 (包括经验回放)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = DQN(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "\n",
    "    # 定义 Agent如何根据State选择Action\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0) # 将状态转换为张量 (还要添加一个维度), 以便输入到网络中\n",
    "        \n",
    "        # 使用 ε-greedy 策略选择动作\n",
    "        if np.random.rand() <= self.epsilon: # 如果随机数小于 ε, 则随机选择一个动作, 用于探索\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # 得到Q值, 用于选择动作\n",
    "        self.model.eval() # 将模型设置为评估模式, 这样可以避免在评估模型时进行梯度更新\n",
    "        with torch.no_grad(): # 不需要计算梯度, 因为我们只是在评估模型\n",
    "            action_values = self.model(state) # 用当前状态获取每个动作的Q值\n",
    "            \n",
    "        self.model.train() # 修改回训练模式, 以便在训练模型时进行梯度更新 (模型的参数可以继续更新)\n",
    "        return np.argmax(action_values.cpu().data.numpy()) # 根据Q值选择最佳动作\n",
    "    \n",
    "    # 用于 经验回放 (Experience Replay)\n",
    "    # 当Agent在Environment中执行Action 并观察到新的状态和奖励时, 将这些信息存储, 之后用于训练网络模型\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # 经验回放 (Experience Replay)\n",
    "    # 打破数据之间的相关性, 提高训练的稳定性\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size: # 如果记忆库中的样本数量小于批量大小, 则不执行\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size) # 从记忆库中随机选择一个批量的经验\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch) # 将批量经验拆分为状态, 动作, 奖励, 下一个状态, 完成标志\n",
    "        # 将拆分的经验转换为张量, 以便输入到网络中\n",
    "        states = torch.tensor(states).float()\n",
    "        actions = torch.tensor(actions)\n",
    "        rewards = torch.tensor(rewards)\n",
    "        next_states = torch.tensor(next_states).float()\n",
    "        dones = torch.tensor(dones, dtype=torch.bool)\n",
    "\n",
    "        Q_targets_next = self.model(next_states).detach().max(1)[0].unsqueeze(1) # 使用目标网络计算下一个状态的Q值, 用于计算目标Q值\n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones)) # 计算目标Q值, 用于更新当前状态的Q值\n",
    "        Q_expected = self.model(states).gather(1, actions.unsqueeze(1)) # 计算预期Q值, 用于计算损失\n",
    "\n",
    "        loss = nn.MSELoss()(Q_expected, Q_targets) # 计算均方误差损失\n",
    "        self.optimizer.zero_grad() # 梯度清零, 以便在每次迭代中重新计算梯度\n",
    "        loss.backward() # 反向传播, 计算梯度\n",
    "        self.optimizer.step() # 更新网络参数\n",
    "\n",
    "        if self.epsilon > self.epsilon_min: # 更新 ε, 以便在训练过程中逐渐减小探索的概率, 以便在初期更多地探索, 在后期更多地利用经验\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LUNARLANDER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
