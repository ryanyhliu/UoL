{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前期准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# 创建Lunar Lander环境，这里使用离散动作空间的版本\n",
    "env = gym.make(\"LunarLander-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索环境\n",
    "了解 动作空间, 观察空间\n",
    "\n",
    "后续 代理设计 需要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Action Space:\", env.action_space)\n",
    "print(\"Observation Space:\", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例代码\n",
    "\n",
    "随机动作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    cumulative_reward = 0\n",
    "    \n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    cumulative_reward += reward\n",
    "    \n",
    "    # print(\"{}, {}, {}, {}\".format(observation, reward, terminated, truncated))\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(\"Episode finished after {} timesteps\".format(_+1), \"Cumulative Reward: \", cumulative_reward)\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 显卡测试\n",
    "\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "pip install torch==2.2.2+cu118 torchvision==0.14.2+cu118 torchaudio==0.12.2+cu118 -f https://download.pytorch.org/whl/torch_stable.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version: \", torch.__version__)\n",
    "print(\"CUDA version: \", torch.version.cuda)\n",
    "\n",
    "# 输出是否可以使用 CUDA\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正式工作 (使用DQN)\n",
    "\n",
    "训练一个Agent, 根据Observation和info, 决定最佳Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 导入并初始化环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 定义DQN网络模型\n",
    "\n",
    "通过PyTorch的nn模块, 来构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"state_size: 状态空间的维度\"\n",
    "        \"action_size: 动作空间的维度\"\n",
    "        super(DQN, self).__init__()\n",
    "        # 定义神经网络的结构\n",
    "        # 输入层的维度是状态空间的维度, 输出层的维度是动作空间的维度\n",
    "        # 输入层 -(fc1)> 128 -(fc2)> 64 -(fc3)> 输出层 (常见的三层全连接神经网络)\n",
    "        # fc1, fc2, fc3 分别是三个全连接层, 用于处理输入数据\n",
    "        self.fc1 = nn.Linear(state_size, 128) \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "        self.relu = nn.ReLU() # relu 是激活函数, 用于增加网络的非线性 (可以在每个全连接层后面添加, 是一个超参数)\n",
    "\n",
    "    # 前向传播: 计算出网络的输出和损失, 用来更新网络\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x) # 返回一个向量, 其维度 = 动作空间的维度. i.e., 每个动作的Q值, 即给定状态下动作的价值(分数 / 预期回报)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        # 增加更多的隐藏层和神经元\n",
    "        self.fc1 = nn.Linear(state_size, 256)  # 第一个隐藏层, 增加到256个神经元\n",
    "        self.fc2 = nn.Linear(256, 128)  # 第二个隐藏层, 128个神经元\n",
    "        self.fc3 = nn.Linear(128, 128)  # 第三个隐藏层, 也是128个神经元\n",
    "        self.fc4 = nn.Linear(128, 64)   # 第四个隐藏层, 64个神经元\n",
    "        self.fc5 = nn.Linear(64, action_size)  # 输出层\n",
    "        self.relu = nn.ReLU()  # 使用ReLU激活函数\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 应用ReLU激活函数到每一个隐藏层\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.fc5(x)  # 输出层不用激活函数，直接返回动作的Q值\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 实现DQN Agent\n",
    "\n",
    "创建一个Agent类, 用来实现DQN的训练 (包括经验回放)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 确定设备\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.model = DQN(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "\n",
    "    # 定义 Agent如何根据State选择Action\n",
    "    def act(self, state):\n",
    "        # state = torch.from_numpy(state).float().unsqueeze(0).to(self.device) # 将状态转换为张量 (还要添加一个维度), 以便输入到网络中\n",
    "        \n",
    "        # 使用 ε-greedy 策略选择动作\n",
    "        if np.random.rand() <= self.epsilon: # 如果随机数小于 ε, 则随机选择一个动作, 用于探索\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # 得到Q值, 用于选择动作\n",
    "        self.model.eval() # 将模型设置为评估模式, 这样可以避免在评估模型时进行梯度更新\n",
    "        with torch.no_grad(): # 不需要计算梯度, 因为我们只是在评估模型\n",
    "            action_values = self.model(state) # 用当前状态获取每个动作的Q值\n",
    "            \n",
    "        self.model.train() # 修改回训练模式, 以便在训练模型时进行梯度更新 (模型的参数可以继续更新)\n",
    "        return np.argmax(action_values.cpu().data.numpy()) # 根据Q值选择最佳动作\n",
    "    \n",
    "    # 用于 经验回放 (Experience Replay)\n",
    "    # 当Agent在Environment中执行Action 并观察到新的状态和奖励时, 将这些信息存储, 之后用于训练网络模型\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # 经验回放 (Experience Replay)\n",
    "    # 打破数据之间的相关性, 提高训练的稳定性\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size: # 如果记忆库中的样本数量小于批量大小, 则不执行\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size) # 从记忆库中随机选择一个批量的经验\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch) # 将批量经验拆分为状态, 动作, 奖励, 下一个状态, 完成标志\n",
    "        # 将拆分的经验转换为张量, 以便输入到网络中\n",
    "        states = torch.tensor(states, dtype =torch.float32).to(self.device).squeeze(1)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(self.device).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device).unsqueeze(1)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device).squeeze(1)\n",
    "        dones = torch.tensor(dones, dtype=torch.bool).to(self.device).unsqueeze(1)\n",
    "\n",
    "        # 打印所有张量的形状\n",
    "        # print(\"states: \", states.shape)\n",
    "        # print(\"actions: \", actions.shape)\n",
    "        # print(\"rewards: \", rewards)\n",
    "        # print(\"next_states: \", next_states.shape)\n",
    "        # print(\"dones: \", dones.shape)\n",
    "\n",
    "        Q_targets_next = self.model(next_states).detach().max(1)[0].unsqueeze(1) # 使用目标网络计算下一个状态的Q值, 用于计算目标Q值\n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next * (~dones)) # 计算目标Q值, 用于更新当前状态的Q值\n",
    "        Q_expected = self.model(states).gather(1, actions) # 计算预期Q值, 用于计算损失\n",
    "\n",
    "        loss = nn.MSELoss()(Q_expected, Q_targets) # 计算均方误差损失\n",
    "        self.optimizer.zero_grad() # 梯度清零, 以便在每次迭代中重新计算梯度\n",
    "        loss.backward() # 反向传播, 计算梯度\n",
    "        self.optimizer.step() # 更新网络参数\n",
    "\n",
    "        if self.epsilon > self.epsilon_min: # 更新 ε, 以便在训练过程中逐渐减小探索的概率, 以便在初期更多地探索, 在后期更多地利用经验\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 训练Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "model_dir = os.path.join(current_dir, \"models\")\n",
    "os.makedirs(model_dir, exist_ok=True) # 创建模型保存目录\n",
    "\n",
    "env.action_space.n\n",
    "agent = DQNAgent(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "agent.model.to(device)  # 移动模型到GPU\n",
    "\n",
    "for e in range(5000): # 训练1000个episode\n",
    "    full_state = env.reset()\n",
    "    state = full_state[0] # 提取向量\n",
    "    # print(\"初始状态\", state)\n",
    "    state = torch.from_numpy(np.reshape(state, [1, -1])).float().to(device) # 把state转换为 网络模型 接受的形状\n",
    "    \n",
    "    cumulative_reward = 0  # 初始化累积奖励为0\n",
    "    \n",
    "    for time in range(1000): # 一个episode最多执行500个时间步\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, truncated, info = env.step(action)  # 执行动作, 获取下一个状态, 奖励, 完成标志, 和\n",
    "        cumulative_reward += reward # 累积奖励\n",
    "        \n",
    "        # print(\"下一个状态\", next_state)\n",
    "        next_state = torch.from_numpy(np.reshape(next_state, [1, -1])).float().to(device) # 把next_state转换为 网络模型 接受的形状\n",
    "        reward = torch.tensor([reward], device=device) # 把reward转换为张量\n",
    "        done = torch.tensor([done], device=device)\n",
    "        \n",
    "        # agent.store(state, action, reward, next_state, done) # 存储经验\n",
    "        agent.store(state.cpu().numpy(), action, reward.cpu().numpy(), next_state.cpu().numpy(), done.cpu().numpy()) # 存储经验\n",
    "        state = next_state # 更新状态\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode: {}/{}, score: {}, cumulative reward: {}\".format(e, 1000, time, cumulative_reward)) # 打印每个episode的时间步数和累积奖励\n",
    "            break\n",
    "        if len(agent.memory) > batch_size: # 当记忆库中的样本数量大于32时, 开始经验回放\n",
    "            agent.replay(batch_size)\n",
    "    \n",
    "    # 每100个episode结束后, 保存模型的状态字典\n",
    "    if e % 100 == 0:\n",
    "        model_path = os.path.join(model_dir, f\"model_0.pth\")  # 使用 f-string 包含 episode 数\n",
    "        torch.save(agent.model.state_dict(), model_path)\n",
    "        print(\"模型状态字典已保存至\", model_path)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DQN:\n\tMissing key(s) in state_dict: \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\". \n\tUnexpected key(s) in state_dict: \"layers.0.weight\", \"layers.0.bias\", \"layers.1.weight\", \"layers.1.bias\", \"layers.2.weight\", \"layers.2.bias\", \"layers.3.weight\", \"layers.3.bias\", \"layers.4.weight\", \"layers.4.bias\", \"layers.5.weight\", \"layers.5.bias\", \"layers.6.weight\", \"layers.6.bias\", \"layers.7.weight\", \"layers.7.bias\", \"layers.8.weight\", \"layers.8.bias\", \"layers.9.weight\", \"layers.9.bias\", \"layers.10.weight\", \"layers.10.bias\", \"layers.11.weight\", \"layers.11.bias\", \"layers.12.weight\", \"layers.12.bias\", \"layers.13.weight\", \"layers.13.bias\", \"layers.14.weight\", \"layers.14.bias\", \"layers.15.weight\", \"layers.15.bias\", \"layers.16.weight\", \"layers.16.bias\", \"layers.17.weight\", \"layers.17.bias\", \"layers.18.weight\", \"layers.18.bias\", \"layers.19.weight\", \"layers.19.bias\", \"layers.20.weight\", \"layers.20.bias\", \"layers.21.weight\", \"layers.21.bias\", \"layers.22.weight\", \"layers.22.bias\", \"layers.23.weight\", \"layers.23.bias\", \"layers.24.weight\", \"layers.24.bias\", \"layers.25.weight\", \"layers.25.bias\", \"layers.26.weight\", \"layers.26.bias\", \"layers.27.weight\", \"layers.27.bias\", \"layers.28.weight\", \"layers.28.bias\", \"layers.29.weight\", \"layers.29.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_0.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m DQN(state_size, action_size)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# 将模型设置为评估模式\u001b[39;00m\n\u001b[0;32m     19\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLunarLander-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\LUNARLANDER\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DQN:\n\tMissing key(s) in state_dict: \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\". \n\tUnexpected key(s) in state_dict: \"layers.0.weight\", \"layers.0.bias\", \"layers.1.weight\", \"layers.1.bias\", \"layers.2.weight\", \"layers.2.bias\", \"layers.3.weight\", \"layers.3.bias\", \"layers.4.weight\", \"layers.4.bias\", \"layers.5.weight\", \"layers.5.bias\", \"layers.6.weight\", \"layers.6.bias\", \"layers.7.weight\", \"layers.7.bias\", \"layers.8.weight\", \"layers.8.bias\", \"layers.9.weight\", \"layers.9.bias\", \"layers.10.weight\", \"layers.10.bias\", \"layers.11.weight\", \"layers.11.bias\", \"layers.12.weight\", \"layers.12.bias\", \"layers.13.weight\", \"layers.13.bias\", \"layers.14.weight\", \"layers.14.bias\", \"layers.15.weight\", \"layers.15.bias\", \"layers.16.weight\", \"layers.16.bias\", \"layers.17.weight\", \"layers.17.bias\", \"layers.18.weight\", \"layers.18.bias\", \"layers.19.weight\", \"layers.19.bias\", \"layers.20.weight\", \"layers.20.bias\", \"layers.21.weight\", \"layers.21.bias\", \"layers.22.weight\", \"layers.22.bias\", \"layers.23.weight\", \"layers.23.bias\", \"layers.24.weight\", \"layers.24.bias\", \"layers.25.weight\", \"layers.25.bias\", \"layers.26.weight\", \"layers.26.bias\", \"layers.27.weight\", \"layers.27.bias\", \"layers.28.weight\", \"layers.28.bias\", \"layers.29.weight\", \"layers.29.bias\". "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import torch# 确保从包含DQN类定义的文件中导入\n",
    "import os\n",
    "\n",
    "# 假设状态空间和动作空间的维度已知\n",
    "state_size = 8  # 根据你的环境设置\n",
    "action_size = 4  # 根据你的环境设置\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "model_dir = os.path.join(current_dir, \"models\")\n",
    "model_path = os.path.join(model_dir, \"model_0.pth\")\n",
    "\n",
    "model = DQN(state_size, action_size)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()  # 将模型设置为评估模式\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "\n",
    "\n",
    "for _ in range(1000):\n",
    "    # 将观测转换为适合模型的格式\n",
    "    state = torch.tensor([observation], dtype=torch.float32)\n",
    "    \n",
    "    cumulative_reward = 0  # 初始化累积奖励为0\n",
    "    \n",
    "    with torch.no_grad():  # 禁止torch追踪此处的梯度计算，因为我们在推理而不是训练\n",
    "        action = model(state).max(1)[1].item()  # 获取最大Q值对应的动作\n",
    "\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    # print(\"reward: \", reward)\n",
    "    cumulative_reward += reward # 累积奖励\n",
    "    \n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(\"Episode finished, reward: \", cumulative_reward)\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-LUNARLANDER]",
   "language": "python",
   "name": "conda-env-.conda-LUNARLANDER-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
