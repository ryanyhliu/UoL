{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前期准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# 创建Lunar Lander环境，这里使用离散动作空间的版本\n",
    "env = gym.make(\"LunarLander-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索环境\n",
    "了解 动作空间, 观察空间\n",
    "\n",
    "后续 代理设计 需要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(4)\n",
      "Observation Space: Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
      " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
      " 1.       ], (8,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space:\", env.action_space)\n",
    "print(\"Observation Space:\", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例代码\n",
    "\n",
    "随机动作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 77 timesteps Cumulative Reward:  -100\n",
      "Episode finished after 149 timesteps Cumulative Reward:  -100\n",
      "Episode finished after 252 timesteps Cumulative Reward:  -100\n",
      "Episode finished after 343 timesteps Cumulative Reward:  -100\n",
      "Episode finished after 438 timesteps Cumulative Reward:  -100\n",
      "Episode finished after 530 timesteps Cumulative Reward:  -100\n",
      "Episode finished after 618 timesteps Cumulative Reward:  -100\n",
      "Episode finished after 745 timesteps Cumulative Reward:  -100\n",
      "Episode finished after 825 timesteps Cumulative Reward:  -100\n",
      "Episode finished after 960 timesteps Cumulative Reward:  -100\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    cumulative_reward = 0\n",
    "    \n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    cumulative_reward += reward\n",
    "    \n",
    "    # print(\"{}, {}, {}, {}\".format(observation, reward, terminated, truncated))\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(\"Episode finished after {} timesteps\".format(_+1), \"Cumulative Reward: \", cumulative_reward)\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 显卡测试\n",
    "\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "pip install torch==2.2.2+cu118 torchvision==0.14.2+cu118 torchaudio==0.12.2+cu118 -f https://download.pytorch.org/whl/torch_stable.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:  2.2.2+cu118\n",
      "CUDA version:  11.8\n",
      "CUDA available:  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version: \", torch.__version__)\n",
    "print(\"CUDA version: \", torch.version.cuda)\n",
    "\n",
    "# 输出是否可以使用 CUDA\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正式工作 (使用DQN)\n",
    "\n",
    "训练一个Agent, 根据Observation和info, 决定最佳Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 导入并初始化环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 定义DQN网络模型\n",
    "\n",
    "通过PyTorch的nn模块, 来构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"state_size: 状态空间的维度\"\n",
    "        \"action_size: 动作空间的维度\"\n",
    "        super(DQN, self).__init__()\n",
    "        # 定义神经网络的结构\n",
    "        # 输入层的维度是状态空间的维度, 输出层的维度是动作空间的维度\n",
    "        # 输入层 -(fc1)> 128 -(fc2)> 64 -(fc3)> 输出层 (常见的三层全连接神经网络)\n",
    "        # fc1, fc2, fc3 分别是三个全连接层, 用于处理输入数据\n",
    "        self.fc1 = nn.Linear(state_size, 128) \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "        self.relu = nn.ReLU() # relu 是激活函数, 用于增加网络的非线性 (可以在每个全连接层后面添加, 是一个超参数)\n",
    "\n",
    "    # 前向传播: 计算出网络的输出和损失, 用来更新网络\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x) # 返回一个向量, 其维度 = 动作空间的维度. i.e., 每个动作的Q值, 即给定状态下动作的价值(分数 / 预期回报)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layers = nn.ModuleList()  # 使用ModuleList来存储所有层\n",
    "\n",
    "        # 第一个层将输入尺寸从state_size转换到8\n",
    "        self.layers.append(nn.Linear(state_size, 8))\n",
    "        \n",
    "        # 添加28个隐藏层，每个都有8个神经元\n",
    "        for _ in range(28):\n",
    "            self.layers.append(nn.Linear(8, 8))\n",
    "        \n",
    "        # 最后一个层将尺寸从8转换到action_size\n",
    "        self.layers.append(nn.Linear(8, action_size))\n",
    "        \n",
    "        # ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 应用所有隐藏层\n",
    "        for layer in self.layers[:-1]:  # 除了最后一个层之外的所有层\n",
    "            x = self.relu(layer(x))\n",
    "        \n",
    "        # 最后一层不使用激活函数\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "# 假设 state_size 和 action_size 已经定义\n",
    "state_size = 10  # 例如\n",
    "action_size = 4  # 例如\n",
    "model = DQN(state_size, action_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 实现DQN Agent\n",
    "\n",
    "创建一个Agent类, 用来实现DQN的训练 (包括经验回放)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 确定设备\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.model = DQN(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "\n",
    "    # 定义 Agent如何根据State选择Action\n",
    "    def act(self, state):\n",
    "        # state = torch.from_numpy(state).float().unsqueeze(0).to(self.device) # 将状态转换为张量 (还要添加一个维度), 以便输入到网络中\n",
    "        \n",
    "        # 使用 ε-greedy 策略选择动作\n",
    "        if np.random.rand() <= self.epsilon: # 如果随机数小于 ε, 则随机选择一个动作, 用于探索\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # 得到Q值, 用于选择动作\n",
    "        self.model.eval() # 将模型设置为评估模式, 这样可以避免在评估模型时进行梯度更新\n",
    "        with torch.no_grad(): # 不需要计算梯度, 因为我们只是在评估模型\n",
    "            action_values = self.model(state) # 用当前状态获取每个动作的Q值\n",
    "            \n",
    "        self.model.train() # 修改回训练模式, 以便在训练模型时进行梯度更新 (模型的参数可以继续更新)\n",
    "        return np.argmax(action_values.cpu().data.numpy()) # 根据Q值选择最佳动作\n",
    "    \n",
    "    # 用于 经验回放 (Experience Replay)\n",
    "    # 当Agent在Environment中执行Action 并观察到新的状态和奖励时, 将这些信息存储, 之后用于训练网络模型\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # 经验回放 (Experience Replay)\n",
    "    # 打破数据之间的相关性, 提高训练的稳定性\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size: # 如果记忆库中的样本数量小于批量大小, 则不执行\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size) # 从记忆库中随机选择一个批量的经验\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch) # 将批量经验拆分为状态, 动作, 奖励, 下一个状态, 完成标志\n",
    "        # 将拆分的经验转换为张量, 以便输入到网络中\n",
    "        states = torch.tensor(states, dtype =torch.float32).to(self.device).squeeze(1)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(self.device).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device).unsqueeze(1)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device).squeeze(1)\n",
    "        dones = torch.tensor(dones, dtype=torch.bool).to(self.device).unsqueeze(1)\n",
    "\n",
    "        # 打印所有张量的形状\n",
    "        # print(\"states: \", states.shape)\n",
    "        # print(\"actions: \", actions.shape)\n",
    "        # print(\"rewards: \", rewards)\n",
    "        # print(\"next_states: \", next_states.shape)\n",
    "        # print(\"dones: \", dones.shape)\n",
    "\n",
    "        Q_targets_next = self.model(next_states).detach().max(1)[0].unsqueeze(1) # 使用目标网络计算下一个状态的Q值, 用于计算目标Q值\n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next * (~dones)) # 计算目标Q值, 用于更新当前状态的Q值\n",
    "        Q_expected = self.model(states).gather(1, actions) # 计算预期Q值, 用于计算损失\n",
    "\n",
    "        loss = nn.MSELoss()(Q_expected, Q_targets) # 计算均方误差损失\n",
    "        self.optimizer.zero_grad() # 梯度清零, 以便在每次迭代中重新计算梯度\n",
    "        loss.backward() # 反向传播, 计算梯度\n",
    "        self.optimizer.step() # 更新网络参数\n",
    "\n",
    "        if self.epsilon > self.epsilon_min: # 更新 ε, 以便在训练过程中逐渐减小探索的概率, 以便在初期更多地探索, 在后期更多地利用经验\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 训练Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isrya\\AppData\\Local\\Temp\\ipykernel_100716\\3236115940.py:52: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  dones = torch.tensor(dones, dtype=torch.bool).to(self.device).unsqueeze(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0/5000, score: 109, cumulative reward: -213.43707621354974\n",
      "模型状态字典已保存至 c:\\Users\\isrya\\#MyFiles\\#MyCode\\GithubUoL\\COMP532\\AS02\\models\\model_0.pth\n",
      "Episode: 1/5000, score: 90, cumulative reward: -79.26393937057925\n",
      "Episode: 2/5000, score: 104, cumulative reward: -216.79360480233754\n",
      "Episode: 3/5000, score: 116, cumulative reward: -109.72640019730702\n",
      "Episode: 4/5000, score: 96, cumulative reward: -130.1678731939367\n",
      "Episode: 5/5000, score: 110, cumulative reward: -470.8044664716901\n",
      "Episode: 6/5000, score: 57, cumulative reward: -96.56335348428325\n",
      "Episode: 7/5000, score: 83, cumulative reward: -154.74424002368482\n",
      "Episode: 8/5000, score: 86, cumulative reward: -348.31746879926675\n",
      "Episode: 9/5000, score: 82, cumulative reward: -453.53823334462913\n",
      "Episode: 10/5000, score: 87, cumulative reward: -338.24233023289344\n",
      "Episode: 11/5000, score: 98, cumulative reward: -136.54658526687822\n",
      "Episode: 12/5000, score: 69, cumulative reward: -173.69516266255397\n",
      "Episode: 13/5000, score: 54, cumulative reward: -107.71101971604479\n",
      "Episode: 14/5000, score: 115, cumulative reward: -122.78233508330831\n",
      "Episode: 15/5000, score: 79, cumulative reward: -120.72448399259105\n",
      "Episode: 16/5000, score: 91, cumulative reward: -375.12967719126743\n",
      "Episode: 17/5000, score: 83, cumulative reward: -126.83374777504991\n",
      "Episode: 18/5000, score: 75, cumulative reward: -69.32711316008198\n",
      "Episode: 19/5000, score: 85, cumulative reward: -122.7029730219096\n",
      "Episode: 20/5000, score: 71, cumulative reward: -215.89249199266135\n",
      "Episode: 21/5000, score: 95, cumulative reward: -266.1217269965381\n",
      "Episode: 22/5000, score: 88, cumulative reward: -497.4507042007787\n",
      "Episode: 23/5000, score: 119, cumulative reward: -290.6980884549313\n",
      "Episode: 24/5000, score: 55, cumulative reward: -141.76727070528074\n",
      "Episode: 25/5000, score: 72, cumulative reward: -174.51694813434528\n",
      "Episode: 26/5000, score: 82, cumulative reward: -160.40564095406353\n",
      "Episode: 27/5000, score: 115, cumulative reward: -21.644933511655523\n",
      "Episode: 28/5000, score: 73, cumulative reward: -339.23296622496434\n",
      "Episode: 29/5000, score: 112, cumulative reward: -248.92688965264887\n",
      "Episode: 30/5000, score: 59, cumulative reward: -120.0388749070692\n",
      "Episode: 31/5000, score: 98, cumulative reward: -329.6572250573568\n",
      "Episode: 32/5000, score: 128, cumulative reward: -519.5403377591867\n",
      "Episode: 33/5000, score: 93, cumulative reward: -409.70772493561003\n",
      "Episode: 34/5000, score: 59, cumulative reward: -105.7020991169318\n",
      "Episode: 35/5000, score: 103, cumulative reward: -242.22499535751945\n",
      "Episode: 36/5000, score: 62, cumulative reward: -199.5739865948857\n",
      "Episode: 37/5000, score: 90, cumulative reward: -338.98568428146376\n",
      "Episode: 38/5000, score: 70, cumulative reward: -438.8303687513933\n",
      "Episode: 39/5000, score: 81, cumulative reward: -193.8554277360769\n",
      "Episode: 40/5000, score: 118, cumulative reward: -259.81689354796214\n",
      "Episode: 41/5000, score: 75, cumulative reward: -160.08965971818887\n",
      "Episode: 42/5000, score: 84, cumulative reward: -303.1254433807897\n",
      "Episode: 43/5000, score: 91, cumulative reward: -113.74636627167834\n",
      "Episode: 44/5000, score: 76, cumulative reward: -192.28538426610757\n",
      "Episode: 45/5000, score: 81, cumulative reward: -341.44658037321045\n",
      "Episode: 46/5000, score: 112, cumulative reward: -122.84829769627336\n",
      "Episode: 47/5000, score: 144, cumulative reward: -250.12190337410038\n",
      "Episode: 48/5000, score: 98, cumulative reward: -85.83060049017392\n",
      "Episode: 49/5000, score: 118, cumulative reward: -35.49781228966684\n",
      "Episode: 50/5000, score: 103, cumulative reward: -233.8925188349797\n",
      "Episode: 51/5000, score: 80, cumulative reward: -197.67643464657078\n",
      "Episode: 52/5000, score: 82, cumulative reward: -259.03778925700783\n",
      "Episode: 53/5000, score: 70, cumulative reward: -259.45777504708406\n",
      "Episode: 54/5000, score: 109, cumulative reward: -302.8798825710488\n",
      "Episode: 55/5000, score: 66, cumulative reward: -109.50899094749974\n",
      "Episode: 56/5000, score: 95, cumulative reward: -589.5334847263492\n",
      "Episode: 57/5000, score: 50, cumulative reward: -103.9827371521726\n",
      "Episode: 58/5000, score: 91, cumulative reward: -89.97793346826523\n",
      "Episode: 59/5000, score: 80, cumulative reward: -393.44611445709825\n",
      "Episode: 60/5000, score: 59, cumulative reward: -283.44426921371075\n",
      "Episode: 61/5000, score: 80, cumulative reward: -78.31084866203936\n",
      "Episode: 62/5000, score: 104, cumulative reward: -330.3582036737164\n",
      "Episode: 63/5000, score: 86, cumulative reward: -287.3557959504825\n",
      "Episode: 64/5000, score: 81, cumulative reward: -218.97410388513157\n",
      "Episode: 65/5000, score: 141, cumulative reward: -112.59322975965517\n",
      "Episode: 66/5000, score: 143, cumulative reward: -709.1210669632435\n",
      "Episode: 67/5000, score: 104, cumulative reward: -375.9265559962886\n",
      "Episode: 68/5000, score: 61, cumulative reward: -166.97700581875685\n",
      "Episode: 69/5000, score: 54, cumulative reward: -79.95655522616363\n",
      "Episode: 70/5000, score: 68, cumulative reward: -182.7571236024423\n",
      "Episode: 71/5000, score: 91, cumulative reward: -489.3130002762883\n",
      "Episode: 72/5000, score: 49, cumulative reward: -159.1065648709502\n",
      "Episode: 73/5000, score: 76, cumulative reward: -242.5775764746721\n",
      "Episode: 74/5000, score: 64, cumulative reward: -76.26692442109619\n",
      "Episode: 75/5000, score: 85, cumulative reward: -89.6485384307386\n",
      "Episode: 76/5000, score: 98, cumulative reward: -499.2470413667828\n",
      "Episode: 77/5000, score: 92, cumulative reward: -358.87178866621844\n",
      "Episode: 78/5000, score: 74, cumulative reward: -423.1648913432856\n",
      "Episode: 79/5000, score: 117, cumulative reward: -159.64153338314745\n",
      "Episode: 80/5000, score: 60, cumulative reward: -187.78524017925002\n",
      "Episode: 81/5000, score: 69, cumulative reward: -487.4743126117487\n",
      "Episode: 82/5000, score: 120, cumulative reward: -327.3014429307984\n",
      "Episode: 83/5000, score: 70, cumulative reward: -266.71277030781346\n",
      "Episode: 84/5000, score: 68, cumulative reward: -260.6659157666041\n",
      "Episode: 85/5000, score: 59, cumulative reward: -119.76035791177925\n",
      "Episode: 86/5000, score: 101, cumulative reward: -391.0047806707367\n",
      "Episode: 87/5000, score: 86, cumulative reward: -181.36508650649728\n",
      "Episode: 88/5000, score: 67, cumulative reward: -60.842737975862654\n",
      "Episode: 89/5000, score: 80, cumulative reward: -523.7408529713907\n",
      "Episode: 90/5000, score: 119, cumulative reward: -497.541796445338\n",
      "Episode: 91/5000, score: 76, cumulative reward: 47.97183003891854\n",
      "Episode: 92/5000, score: 99, cumulative reward: -527.0274272468233\n",
      "Episode: 93/5000, score: 98, cumulative reward: -125.07066179338217\n",
      "Episode: 94/5000, score: 118, cumulative reward: -585.8389474802564\n",
      "Episode: 95/5000, score: 72, cumulative reward: -318.0692574212609\n",
      "Episode: 96/5000, score: 86, cumulative reward: -157.78250698287846\n",
      "Episode: 97/5000, score: 54, cumulative reward: -110.16063434577867\n",
      "Episode: 98/5000, score: 81, cumulative reward: -110.40922210733859\n",
      "Episode: 99/5000, score: 81, cumulative reward: -197.3573442789316\n",
      "Episode: 100/5000, score: 101, cumulative reward: -417.52099866595756\n",
      "模型状态字典已保存至 c:\\Users\\isrya\\#MyFiles\\#MyCode\\GithubUoL\\COMP532\\AS02\\models\\model_0.pth\n",
      "Episode: 101/5000, score: 103, cumulative reward: -217.65091390358873\n",
      "Episode: 102/5000, score: 55, cumulative reward: -355.7255083355915\n",
      "Episode: 103/5000, score: 53, cumulative reward: -402.035735963778\n",
      "Episode: 104/5000, score: 90, cumulative reward: -194.71715585731977\n",
      "Episode: 105/5000, score: 70, cumulative reward: -609.9021978951919\n",
      "Episode: 106/5000, score: 128, cumulative reward: -301.4570575595888\n",
      "Episode: 107/5000, score: 65, cumulative reward: -102.27513761078542\n",
      "Episode: 108/5000, score: 101, cumulative reward: -62.52239901281707\n",
      "Episode: 109/5000, score: 77, cumulative reward: -237.08187877515525\n",
      "Episode: 110/5000, score: 100, cumulative reward: -493.0239239115875\n",
      "Episode: 111/5000, score: 65, cumulative reward: -183.4026210634081\n",
      "Episode: 112/5000, score: 96, cumulative reward: -311.0553512368068\n",
      "Episode: 113/5000, score: 73, cumulative reward: -275.97289246782145\n",
      "Episode: 114/5000, score: 114, cumulative reward: -208.13892180096053\n",
      "Episode: 115/5000, score: 81, cumulative reward: -68.85199863396625\n",
      "Episode: 116/5000, score: 61, cumulative reward: -299.43228300596587\n",
      "Episode: 117/5000, score: 76, cumulative reward: -361.50951190761816\n",
      "Episode: 118/5000, score: 108, cumulative reward: -459.25339238827024\n",
      "Episode: 119/5000, score: 75, cumulative reward: -160.56865162950282\n",
      "Episode: 120/5000, score: 88, cumulative reward: -367.1676312987041\n",
      "Episode: 121/5000, score: 105, cumulative reward: -135.53433766716344\n",
      "Episode: 122/5000, score: 75, cumulative reward: -149.6426615642264\n",
      "Episode: 123/5000, score: 81, cumulative reward: -336.92376948007933\n",
      "Episode: 124/5000, score: 111, cumulative reward: -152.90614145027274\n",
      "Episode: 125/5000, score: 67, cumulative reward: -181.83543039047834\n",
      "Episode: 126/5000, score: 109, cumulative reward: -46.62445071925655\n",
      "Episode: 127/5000, score: 83, cumulative reward: -100.01887452756087\n",
      "Episode: 128/5000, score: 98, cumulative reward: -252.63322038814349\n",
      "Episode: 129/5000, score: 112, cumulative reward: -437.6811486469024\n",
      "Episode: 130/5000, score: 122, cumulative reward: -370.07387718630173\n",
      "Episode: 131/5000, score: 93, cumulative reward: -89.22975147660394\n",
      "Episode: 132/5000, score: 108, cumulative reward: -126.13006322182356\n",
      "Episode: 133/5000, score: 79, cumulative reward: -138.17440676640928\n",
      "Episode: 134/5000, score: 120, cumulative reward: -340.5718144170824\n",
      "Episode: 135/5000, score: 113, cumulative reward: -128.70773736025293\n",
      "Episode: 136/5000, score: 58, cumulative reward: -152.04257854071415\n",
      "Episode: 137/5000, score: 63, cumulative reward: -160.17091639987956\n",
      "Episode: 138/5000, score: 63, cumulative reward: -329.7078247240462\n",
      "Episode: 139/5000, score: 80, cumulative reward: -134.98965344950633\n",
      "Episode: 140/5000, score: 65, cumulative reward: -122.74134948171599\n",
      "Episode: 141/5000, score: 54, cumulative reward: -100.81529414139249\n",
      "Episode: 142/5000, score: 62, cumulative reward: -112.34651549238848\n",
      "Episode: 143/5000, score: 96, cumulative reward: -619.8762506205151\n",
      "Episode: 144/5000, score: 87, cumulative reward: -208.4478690485666\n",
      "Episode: 145/5000, score: 66, cumulative reward: -143.352306795149\n",
      "Episode: 146/5000, score: 103, cumulative reward: -184.58527622140866\n",
      "Episode: 147/5000, score: 143, cumulative reward: -366.09194186814557\n",
      "Episode: 148/5000, score: 57, cumulative reward: -212.7907729557491\n",
      "Episode: 149/5000, score: 92, cumulative reward: -515.206600405111\n",
      "Episode: 150/5000, score: 87, cumulative reward: -123.74454500742229\n",
      "Episode: 151/5000, score: 55, cumulative reward: -265.3470100880411\n",
      "Episode: 152/5000, score: 84, cumulative reward: -339.61281096575885\n",
      "Episode: 153/5000, score: 136, cumulative reward: -376.11572335074635\n",
      "Episode: 154/5000, score: 171, cumulative reward: -243.28969895640327\n",
      "Episode: 155/5000, score: 74, cumulative reward: -346.5648639816617\n",
      "Episode: 156/5000, score: 53, cumulative reward: -151.50916326691566\n",
      "Episode: 157/5000, score: 95, cumulative reward: -446.87717394757794\n",
      "Episode: 158/5000, score: 64, cumulative reward: -181.09789739072338\n",
      "Episode: 159/5000, score: 76, cumulative reward: -120.19694855123527\n",
      "Episode: 160/5000, score: 108, cumulative reward: -142.48086671076072\n",
      "Episode: 161/5000, score: 65, cumulative reward: -267.96409287953657\n",
      "Episode: 162/5000, score: 91, cumulative reward: -251.9481847680867\n",
      "Episode: 163/5000, score: 54, cumulative reward: -119.33182748181108\n",
      "Episode: 164/5000, score: 85, cumulative reward: -402.33764616592373\n",
      "Episode: 165/5000, score: 130, cumulative reward: -556.4715516629831\n",
      "Episode: 166/5000, score: 97, cumulative reward: -117.75790950474793\n",
      "Episode: 167/5000, score: 57, cumulative reward: -94.92485309252612\n",
      "Episode: 168/5000, score: 101, cumulative reward: -149.31881305330026\n",
      "Episode: 169/5000, score: 92, cumulative reward: -274.79417158044777\n",
      "Episode: 170/5000, score: 81, cumulative reward: -493.3804090679949\n",
      "Episode: 171/5000, score: 79, cumulative reward: -421.29604531019396\n",
      "Episode: 172/5000, score: 91, cumulative reward: -59.58466526856313\n",
      "Episode: 173/5000, score: 60, cumulative reward: -327.03128095250656\n",
      "Episode: 174/5000, score: 66, cumulative reward: -274.83076340722505\n",
      "Episode: 175/5000, score: 98, cumulative reward: -482.9568175750955\n",
      "Episode: 176/5000, score: 122, cumulative reward: -516.8364498103366\n",
      "Episode: 177/5000, score: 87, cumulative reward: -76.04821194086819\n",
      "Episode: 178/5000, score: 103, cumulative reward: -369.12542869687707\n",
      "Episode: 179/5000, score: 102, cumulative reward: -19.917584564814277\n",
      "Episode: 180/5000, score: 96, cumulative reward: -99.82036413239436\n",
      "Episode: 181/5000, score: 53, cumulative reward: -117.00685981597377\n",
      "Episode: 182/5000, score: 63, cumulative reward: -202.17076649415168\n",
      "Episode: 183/5000, score: 102, cumulative reward: -361.74838942457535\n",
      "Episode: 184/5000, score: 56, cumulative reward: -185.14118721732896\n",
      "Episode: 185/5000, score: 74, cumulative reward: -228.8315838211741\n",
      "Episode: 186/5000, score: 63, cumulative reward: -250.4194324771058\n",
      "Episode: 187/5000, score: 80, cumulative reward: -556.9096127019404\n",
      "Episode: 188/5000, score: 103, cumulative reward: -38.87450483639334\n",
      "Episode: 189/5000, score: 67, cumulative reward: -105.96040615049262\n",
      "Episode: 190/5000, score: 57, cumulative reward: -159.38440532020488\n",
      "Episode: 191/5000, score: 79, cumulative reward: -262.60838543846523\n",
      "Episode: 192/5000, score: 110, cumulative reward: -469.029081173987\n",
      "Episode: 193/5000, score: 81, cumulative reward: -415.17186710858783\n",
      "Episode: 194/5000, score: 72, cumulative reward: -116.59061788523306\n",
      "Episode: 195/5000, score: 86, cumulative reward: -371.89439899893597\n",
      "Episode: 196/5000, score: 86, cumulative reward: -184.95439537827292\n",
      "Episode: 197/5000, score: 90, cumulative reward: -106.39717369523558\n",
      "Episode: 198/5000, score: 88, cumulative reward: -317.9325652592564\n",
      "Episode: 199/5000, score: 125, cumulative reward: -287.12550689454963\n",
      "Episode: 200/5000, score: 80, cumulative reward: -214.4132497947722\n",
      "模型状态字典已保存至 c:\\Users\\isrya\\#MyFiles\\#MyCode\\GithubUoL\\COMP532\\AS02\\models\\model_0.pth\n",
      "Episode: 201/5000, score: 66, cumulative reward: -294.2150227822882\n",
      "Episode: 202/5000, score: 132, cumulative reward: -435.886820564968\n",
      "Episode: 203/5000, score: 79, cumulative reward: -484.74045501375605\n",
      "Episode: 204/5000, score: 90, cumulative reward: -223.71977997853955\n",
      "Episode: 205/5000, score: 53, cumulative reward: -200.84326702250297\n",
      "Episode: 206/5000, score: 118, cumulative reward: -84.87536041588828\n",
      "Episode: 207/5000, score: 85, cumulative reward: -296.74419749598735\n",
      "Episode: 208/5000, score: 58, cumulative reward: -243.8475383776863\n",
      "Episode: 209/5000, score: 135, cumulative reward: -260.2967412700269\n",
      "Episode: 210/5000, score: 70, cumulative reward: -187.60512704221685\n",
      "Episode: 211/5000, score: 140, cumulative reward: -305.98728292027903\n",
      "Episode: 212/5000, score: 89, cumulative reward: -268.44292294145555\n",
      "Episode: 213/5000, score: 87, cumulative reward: -222.68971649831053\n",
      "Episode: 214/5000, score: 62, cumulative reward: -123.50392780179112\n",
      "Episode: 215/5000, score: 72, cumulative reward: -119.97146698021572\n",
      "Episode: 216/5000, score: 80, cumulative reward: -285.95917224744267\n",
      "Episode: 217/5000, score: 95, cumulative reward: -210.03090054867653\n",
      "Episode: 218/5000, score: 107, cumulative reward: -333.55250358465645\n",
      "Episode: 219/5000, score: 63, cumulative reward: -231.12719809164875\n",
      "Episode: 220/5000, score: 71, cumulative reward: -159.66912623120635\n",
      "Episode: 221/5000, score: 70, cumulative reward: -199.67409805001918\n",
      "Episode: 222/5000, score: 72, cumulative reward: -184.6892757085783\n",
      "Episode: 223/5000, score: 72, cumulative reward: -225.44504416459\n",
      "Episode: 224/5000, score: 123, cumulative reward: -424.76279075972263\n",
      "Episode: 225/5000, score: 112, cumulative reward: -217.04219618542112\n",
      "Episode: 226/5000, score: 77, cumulative reward: -44.93592722649237\n",
      "Episode: 227/5000, score: 83, cumulative reward: -90.76899312037509\n",
      "Episode: 228/5000, score: 106, cumulative reward: -358.7818829286581\n",
      "Episode: 229/5000, score: 101, cumulative reward: -221.07977281735924\n",
      "Episode: 230/5000, score: 81, cumulative reward: -148.79405588955808\n",
      "Episode: 231/5000, score: 64, cumulative reward: -100.98312881171202\n",
      "Episode: 232/5000, score: 108, cumulative reward: -65.2958981528364\n",
      "Episode: 233/5000, score: 77, cumulative reward: -124.3891225183056\n",
      "Episode: 234/5000, score: 88, cumulative reward: -83.75985892831645\n",
      "Episode: 235/5000, score: 84, cumulative reward: -407.5671068503441\n",
      "Episode: 236/5000, score: 84, cumulative reward: -95.00198720767352\n",
      "Episode: 237/5000, score: 80, cumulative reward: -443.0011232987096\n",
      "Episode: 238/5000, score: 122, cumulative reward: -164.78239735066535\n",
      "Episode: 239/5000, score: 137, cumulative reward: -319.1093504329363\n",
      "Episode: 240/5000, score: 99, cumulative reward: -107.56001569911153\n",
      "Episode: 241/5000, score: 87, cumulative reward: -405.004979939534\n",
      "Episode: 242/5000, score: 66, cumulative reward: -62.894425045186374\n",
      "Episode: 243/5000, score: 95, cumulative reward: -181.07428985114302\n",
      "Episode: 244/5000, score: 105, cumulative reward: -125.6189826507632\n",
      "Episode: 245/5000, score: 156, cumulative reward: -243.29421726295251\n",
      "Episode: 246/5000, score: 120, cumulative reward: -116.55293071614234\n",
      "Episode: 247/5000, score: 89, cumulative reward: -88.97722421769667\n",
      "Episode: 248/5000, score: 85, cumulative reward: -138.8782112179613\n",
      "Episode: 249/5000, score: 84, cumulative reward: -464.6505230273555\n",
      "Episode: 250/5000, score: 136, cumulative reward: -249.00030396085282\n",
      "Episode: 251/5000, score: 80, cumulative reward: -315.57586117961125\n",
      "Episode: 252/5000, score: 67, cumulative reward: -164.40162539928525\n",
      "Episode: 253/5000, score: 51, cumulative reward: -149.39344022231504\n",
      "Episode: 254/5000, score: 78, cumulative reward: -121.73384226124885\n",
      "Episode: 255/5000, score: 93, cumulative reward: -361.8801021134346\n",
      "Episode: 256/5000, score: 95, cumulative reward: -335.04213809929746\n",
      "Episode: 257/5000, score: 115, cumulative reward: -130.03477122930914\n",
      "Episode: 258/5000, score: 69, cumulative reward: -72.86127722289895\n",
      "Episode: 259/5000, score: 100, cumulative reward: -337.52120923645214\n",
      "Episode: 260/5000, score: 71, cumulative reward: -396.07639010386765\n",
      "Episode: 261/5000, score: 99, cumulative reward: -240.33083008028808\n",
      "Episode: 262/5000, score: 108, cumulative reward: -162.68453655956677\n",
      "Episode: 263/5000, score: 106, cumulative reward: -141.05076745809896\n",
      "Episode: 264/5000, score: 80, cumulative reward: -417.97548426528743\n",
      "Episode: 265/5000, score: 73, cumulative reward: -56.66322135726063\n",
      "Episode: 266/5000, score: 80, cumulative reward: -297.04645996965394\n",
      "Episode: 267/5000, score: 80, cumulative reward: -76.4281085999072\n",
      "Episode: 268/5000, score: 74, cumulative reward: -170.9857753427309\n",
      "Episode: 269/5000, score: 99, cumulative reward: -298.4826596378675\n",
      "Episode: 270/5000, score: 68, cumulative reward: -226.02253419477952\n",
      "Episode: 271/5000, score: 106, cumulative reward: -327.13590136172513\n",
      "Episode: 272/5000, score: 115, cumulative reward: -322.155027651858\n",
      "Episode: 273/5000, score: 67, cumulative reward: -68.49726405916923\n",
      "Episode: 274/5000, score: 132, cumulative reward: -504.34448100579357\n",
      "Episode: 275/5000, score: 58, cumulative reward: -175.46466992069094\n",
      "Episode: 276/5000, score: 103, cumulative reward: -298.70335603262663\n",
      "Episode: 277/5000, score: 52, cumulative reward: -162.84009799091734\n",
      "Episode: 278/5000, score: 70, cumulative reward: -119.76141692731568\n",
      "Episode: 279/5000, score: 85, cumulative reward: -296.81588611479066\n",
      "Episode: 280/5000, score: 64, cumulative reward: -246.63906044047997\n",
      "Episode: 281/5000, score: 73, cumulative reward: -401.9336212729365\n",
      "Episode: 282/5000, score: 100, cumulative reward: -129.02130592471974\n",
      "Episode: 283/5000, score: 70, cumulative reward: -292.76327706797645\n",
      "Episode: 284/5000, score: 98, cumulative reward: -394.89328321983425\n",
      "Episode: 285/5000, score: 115, cumulative reward: -550.1769510926086\n",
      "Episode: 286/5000, score: 93, cumulative reward: -462.3889821419928\n",
      "Episode: 287/5000, score: 110, cumulative reward: -512.8603002653012\n",
      "Episode: 288/5000, score: 93, cumulative reward: -31.545974937018215\n",
      "Episode: 289/5000, score: 77, cumulative reward: -194.40842362386056\n",
      "Episode: 290/5000, score: 85, cumulative reward: -398.966565164351\n",
      "Episode: 291/5000, score: 81, cumulative reward: -555.2268003621468\n",
      "Episode: 292/5000, score: 109, cumulative reward: -75.0574081551413\n",
      "Episode: 293/5000, score: 72, cumulative reward: -246.50658563931373\n",
      "Episode: 294/5000, score: 80, cumulative reward: -134.41617036592828\n",
      "Episode: 295/5000, score: 76, cumulative reward: -394.6014494761378\n",
      "Episode: 296/5000, score: 102, cumulative reward: -434.7819716684531\n",
      "Episode: 297/5000, score: 61, cumulative reward: -105.21610648491145\n",
      "Episode: 298/5000, score: 65, cumulative reward: -53.23339086945724\n",
      "Episode: 299/5000, score: 60, cumulative reward: -140.30864866249806\n",
      "Episode: 300/5000, score: 69, cumulative reward: -393.28964313091217\n",
      "模型状态字典已保存至 c:\\Users\\isrya\\#MyFiles\\#MyCode\\GithubUoL\\COMP532\\AS02\\models\\model_0.pth\n",
      "Episode: 301/5000, score: 85, cumulative reward: -317.91132824666226\n",
      "Episode: 302/5000, score: 82, cumulative reward: -168.63407340183647\n",
      "Episode: 303/5000, score: 106, cumulative reward: -191.47922839876637\n",
      "Episode: 304/5000, score: 106, cumulative reward: -9.076554487584957\n",
      "Episode: 305/5000, score: 63, cumulative reward: -55.89224801699868\n",
      "Episode: 306/5000, score: 92, cumulative reward: -103.11476167036271\n",
      "Episode: 307/5000, score: 81, cumulative reward: -366.1598126177284\n",
      "Episode: 308/5000, score: 71, cumulative reward: -100.14731975535862\n",
      "Episode: 309/5000, score: 161, cumulative reward: -66.06227917298503\n",
      "Episode: 310/5000, score: 84, cumulative reward: -558.6760194537749\n",
      "Episode: 311/5000, score: 79, cumulative reward: -395.7135105102557\n",
      "Episode: 312/5000, score: 69, cumulative reward: -326.70320340022823\n",
      "Episode: 313/5000, score: 86, cumulative reward: -625.5433082414623\n",
      "Episode: 314/5000, score: 79, cumulative reward: -141.98553225460174\n",
      "Episode: 315/5000, score: 133, cumulative reward: -89.79208449941109\n",
      "Episode: 316/5000, score: 93, cumulative reward: -330.683775831386\n",
      "Episode: 317/5000, score: 100, cumulative reward: -241.23910938008495\n",
      "Episode: 318/5000, score: 65, cumulative reward: -128.84948615258446\n",
      "Episode: 319/5000, score: 114, cumulative reward: -267.4032893986806\n",
      "Episode: 320/5000, score: 67, cumulative reward: -377.50380899129766\n",
      "Episode: 321/5000, score: 90, cumulative reward: -198.62029427904824\n",
      "Episode: 322/5000, score: 95, cumulative reward: -142.856169705331\n",
      "Episode: 323/5000, score: 96, cumulative reward: -420.82446820473376\n",
      "Episode: 324/5000, score: 81, cumulative reward: -215.63788147259126\n",
      "Episode: 325/5000, score: 113, cumulative reward: -366.9653045587651\n",
      "Episode: 326/5000, score: 69, cumulative reward: -443.86209942909227\n",
      "Episode: 327/5000, score: 87, cumulative reward: -413.04481627141763\n",
      "Episode: 328/5000, score: 107, cumulative reward: -231.3295654676557\n",
      "Episode: 329/5000, score: 76, cumulative reward: -115.58717874016212\n",
      "Episode: 330/5000, score: 97, cumulative reward: -294.09176575726275\n",
      "Episode: 331/5000, score: 77, cumulative reward: -473.3809531659602\n",
      "Episode: 332/5000, score: 148, cumulative reward: -425.11055740479867\n",
      "Episode: 333/5000, score: 52, cumulative reward: -98.54852751926225\n",
      "Episode: 334/5000, score: 107, cumulative reward: -138.76339817098253\n",
      "Episode: 335/5000, score: 107, cumulative reward: -166.35135125764592\n",
      "Episode: 336/5000, score: 90, cumulative reward: -391.1079083623004\n",
      "Episode: 337/5000, score: 56, cumulative reward: -184.53154895465053\n",
      "Episode: 338/5000, score: 67, cumulative reward: -171.43266113982946\n",
      "Episode: 339/5000, score: 108, cumulative reward: -184.7202654501143\n",
      "Episode: 340/5000, score: 100, cumulative reward: -411.5708640851181\n",
      "Episode: 341/5000, score: 106, cumulative reward: -642.5135688096342\n",
      "Episode: 342/5000, score: 66, cumulative reward: -100.49771629293099\n",
      "Episode: 343/5000, score: 108, cumulative reward: -149.65425296542105\n",
      "Episode: 344/5000, score: 106, cumulative reward: -452.53877396368375\n",
      "Episode: 345/5000, score: 92, cumulative reward: -437.3450146894045\n",
      "Episode: 346/5000, score: 88, cumulative reward: -176.3528198206024\n",
      "Episode: 347/5000, score: 96, cumulative reward: -165.22193159613545\n",
      "Episode: 348/5000, score: 110, cumulative reward: -132.04359790363958\n",
      "Episode: 349/5000, score: 87, cumulative reward: -176.82787633107904\n",
      "Episode: 350/5000, score: 61, cumulative reward: -233.21820606859913\n",
      "Episode: 351/5000, score: 94, cumulative reward: -110.7767904235319\n",
      "Episode: 352/5000, score: 75, cumulative reward: -451.86661252224\n",
      "Episode: 353/5000, score: 120, cumulative reward: -36.632337524868916\n",
      "Episode: 354/5000, score: 89, cumulative reward: -271.2637252538037\n",
      "Episode: 355/5000, score: 80, cumulative reward: -475.71221272656663\n",
      "Episode: 356/5000, score: 102, cumulative reward: -12.45637494551977\n",
      "Episode: 357/5000, score: 75, cumulative reward: -244.3253771137382\n",
      "Episode: 358/5000, score: 62, cumulative reward: -200.34830347180653\n",
      "Episode: 359/5000, score: 109, cumulative reward: -350.57944067579535\n",
      "Episode: 360/5000, score: 67, cumulative reward: -290.17940526922035\n",
      "Episode: 361/5000, score: 81, cumulative reward: -315.5250804149822\n",
      "Episode: 362/5000, score: 96, cumulative reward: -196.94793993231366\n",
      "Episode: 363/5000, score: 62, cumulative reward: -91.49685940440456\n",
      "Episode: 364/5000, score: 97, cumulative reward: -319.11168767390427\n",
      "Episode: 365/5000, score: 81, cumulative reward: -440.14593105420283\n",
      "Episode: 366/5000, score: 95, cumulative reward: -285.6452535654671\n",
      "Episode: 367/5000, score: 116, cumulative reward: -564.9442200534605\n",
      "Episode: 368/5000, score: 100, cumulative reward: -342.6334852102606\n",
      "Episode: 369/5000, score: 70, cumulative reward: -173.5214061239119\n",
      "Episode: 370/5000, score: 99, cumulative reward: -323.9543737733653\n",
      "Episode: 371/5000, score: 94, cumulative reward: -228.36795418101966\n",
      "Episode: 372/5000, score: 55, cumulative reward: -103.58527605247474\n",
      "Episode: 373/5000, score: 65, cumulative reward: -305.98571379666475\n",
      "Episode: 374/5000, score: 141, cumulative reward: -68.54042032024378\n",
      "Episode: 375/5000, score: 102, cumulative reward: -567.8149158320205\n",
      "Episode: 376/5000, score: 70, cumulative reward: -240.63077979226088\n",
      "Episode: 377/5000, score: 72, cumulative reward: -81.2406246205491\n",
      "Episode: 378/5000, score: 60, cumulative reward: -343.3834447618753\n",
      "Episode: 379/5000, score: 115, cumulative reward: -455.57099179913723\n",
      "Episode: 380/5000, score: 84, cumulative reward: -288.1342149676519\n",
      "Episode: 381/5000, score: 92, cumulative reward: -385.36469806720737\n",
      "Episode: 382/5000, score: 58, cumulative reward: -326.0421280793854\n",
      "Episode: 383/5000, score: 74, cumulative reward: -303.18749544236675\n",
      "Episode: 384/5000, score: 114, cumulative reward: -550.7706055490912\n",
      "Episode: 385/5000, score: 61, cumulative reward: -123.00941458147966\n",
      "Episode: 386/5000, score: 101, cumulative reward: -300.90569604236293\n",
      "Episode: 387/5000, score: 78, cumulative reward: -190.90798064300157\n",
      "Episode: 388/5000, score: 69, cumulative reward: -213.8006505332718\n",
      "Episode: 389/5000, score: 83, cumulative reward: -122.18292943337926\n",
      "Episode: 390/5000, score: 95, cumulative reward: -366.69180077330975\n",
      "Episode: 391/5000, score: 62, cumulative reward: -132.67333222004373\n",
      "Episode: 392/5000, score: 59, cumulative reward: -276.30602365459436\n",
      "Episode: 393/5000, score: 64, cumulative reward: -97.90264068244225\n",
      "Episode: 394/5000, score: 60, cumulative reward: -51.27114252392559\n",
      "Episode: 395/5000, score: 129, cumulative reward: -227.47903646345995\n",
      "Episode: 396/5000, score: 78, cumulative reward: -112.98730675163335\n",
      "Episode: 397/5000, score: 72, cumulative reward: -109.7156069236403\n",
      "Episode: 398/5000, score: 62, cumulative reward: -214.15093564246973\n",
      "Episode: 399/5000, score: 105, cumulative reward: -16.413991252683005\n",
      "Episode: 400/5000, score: 96, cumulative reward: -542.6507532007186\n",
      "模型状态字典已保存至 c:\\Users\\isrya\\#MyFiles\\#MyCode\\GithubUoL\\COMP532\\AS02\\models\\model_0.pth\n",
      "Episode: 401/5000, score: 73, cumulative reward: -150.92411820321198\n",
      "Episode: 402/5000, score: 59, cumulative reward: -129.07249574046273\n",
      "Episode: 403/5000, score: 91, cumulative reward: -260.47998632897287\n",
      "Episode: 404/5000, score: 71, cumulative reward: -570.7518641492215\n",
      "Episode: 405/5000, score: 81, cumulative reward: -77.69870200982298\n",
      "Episode: 406/5000, score: 85, cumulative reward: -504.86879373808296\n",
      "Episode: 407/5000, score: 70, cumulative reward: -160.1679334883158\n",
      "Episode: 408/5000, score: 100, cumulative reward: -247.41856226434933\n",
      "Episode: 409/5000, score: 69, cumulative reward: -209.46124384892772\n",
      "Episode: 410/5000, score: 84, cumulative reward: -353.77971172265916\n",
      "Episode: 411/5000, score: 71, cumulative reward: 18.081363102097527\n",
      "Episode: 412/5000, score: 53, cumulative reward: -64.65257755818493\n",
      "Episode: 413/5000, score: 155, cumulative reward: -386.6517532353416\n",
      "Episode: 414/5000, score: 96, cumulative reward: -352.3531982510042\n",
      "Episode: 415/5000, score: 60, cumulative reward: -107.0812397794886\n",
      "Episode: 416/5000, score: 95, cumulative reward: -297.4586007203011\n",
      "Episode: 417/5000, score: 75, cumulative reward: -475.9055304596549\n",
      "Episode: 418/5000, score: 64, cumulative reward: -197.65089634754185\n",
      "Episode: 419/5000, score: 86, cumulative reward: -282.2801485129058\n",
      "Episode: 420/5000, score: 76, cumulative reward: -361.6292561392892\n",
      "Episode: 421/5000, score: 78, cumulative reward: -294.2400083942032\n",
      "Episode: 422/5000, score: 142, cumulative reward: -228.5814487653571\n",
      "Episode: 423/5000, score: 89, cumulative reward: -188.96427276755347\n",
      "Episode: 424/5000, score: 121, cumulative reward: -402.85228209732344\n",
      "Episode: 425/5000, score: 100, cumulative reward: -205.49893433292\n",
      "Episode: 426/5000, score: 73, cumulative reward: -149.7514984119441\n",
      "Episode: 427/5000, score: 96, cumulative reward: -451.5428320347507\n",
      "Episode: 428/5000, score: 81, cumulative reward: -326.12455455814995\n",
      "Episode: 429/5000, score: 65, cumulative reward: -154.37793209928174\n",
      "Episode: 430/5000, score: 73, cumulative reward: -133.3273385090114\n",
      "Episode: 431/5000, score: 98, cumulative reward: -51.836259440705675\n",
      "Episode: 432/5000, score: 94, cumulative reward: -355.38442374377496\n",
      "Episode: 433/5000, score: 77, cumulative reward: -501.9020905267706\n",
      "Episode: 434/5000, score: 56, cumulative reward: -158.22828607384025\n",
      "Episode: 435/5000, score: 106, cumulative reward: -107.14686632275547\n",
      "Episode: 436/5000, score: 111, cumulative reward: -438.01276113175436\n",
      "Episode: 437/5000, score: 105, cumulative reward: -404.7547911716072\n",
      "Episode: 438/5000, score: 77, cumulative reward: -101.72704570153948\n",
      "Episode: 439/5000, score: 81, cumulative reward: -340.0040012603539\n",
      "Episode: 440/5000, score: 137, cumulative reward: -102.65113886496145\n",
      "Episode: 441/5000, score: 55, cumulative reward: -110.32332222408664\n",
      "Episode: 442/5000, score: 70, cumulative reward: -421.3148504689353\n",
      "Episode: 443/5000, score: 76, cumulative reward: -279.61790197483424\n",
      "Episode: 444/5000, score: 86, cumulative reward: -108.97142053739093\n",
      "Episode: 445/5000, score: 90, cumulative reward: -388.2991817696591\n",
      "Episode: 446/5000, score: 98, cumulative reward: -397.5350804057655\n",
      "Episode: 447/5000, score: 68, cumulative reward: -258.29160899064783\n",
      "Episode: 448/5000, score: 57, cumulative reward: -110.18706058348218\n",
      "Episode: 449/5000, score: 65, cumulative reward: -365.0259163604325\n",
      "Episode: 450/5000, score: 96, cumulative reward: -458.63570763512735\n",
      "Episode: 451/5000, score: 109, cumulative reward: -476.7264053142657\n",
      "Episode: 452/5000, score: 84, cumulative reward: -272.0854641227704\n",
      "Episode: 453/5000, score: 67, cumulative reward: -159.79731893262817\n",
      "Episode: 454/5000, score: 88, cumulative reward: -280.6715433412178\n",
      "Episode: 455/5000, score: 92, cumulative reward: -357.18715697327525\n",
      "Episode: 456/5000, score: 68, cumulative reward: -150.9247947722888\n",
      "Episode: 457/5000, score: 112, cumulative reward: -233.37828434531005\n",
      "Episode: 458/5000, score: 98, cumulative reward: -159.10545110367212\n",
      "Episode: 459/5000, score: 79, cumulative reward: -294.9635391784207\n",
      "Episode: 460/5000, score: 117, cumulative reward: -183.3645962356048\n",
      "Episode: 461/5000, score: 119, cumulative reward: -500.2420944983516\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "model_dir = os.path.join(current_dir, \"models\")\n",
    "os.makedirs(model_dir, exist_ok=True) # 创建模型保存目录\n",
    "\n",
    "\n",
    "agent = DQNAgent(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "agent.model.to(device)  # 移动模型到GPU\n",
    "\n",
    "# 定义训练参数\n",
    "e_range = 5000\n",
    "time_range = 200\n",
    "\n",
    "for e in range(e_range): # 训练1000个episode\n",
    "    full_state = env.reset()\n",
    "    state = full_state[0] # 提取向量\n",
    "    # print(\"初始状态\", state)\n",
    "    state = torch.from_numpy(np.reshape(state, [1, -1])).float().to(device) # 把state转换为 网络模型 接受的形状\n",
    "    \n",
    "    cumulative_reward = 0  # 初始化累积奖励为0\n",
    "    \n",
    "    for time in range(time_range): # 一个episode最多执行500个时间步\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, truncated, info = env.step(action)  # 执行动作, 获取下一个状态, 奖励, 完成标志, 和\n",
    "        cumulative_reward += reward # 累积奖励\n",
    "        \n",
    "        # print(\"下一个状态\", next_state)\n",
    "        next_state = torch.from_numpy(np.reshape(next_state, [1, -1])).float().to(device) # 把next_state转换为 网络模型 接受的形状\n",
    "        reward = torch.tensor([reward], device=device) # 把reward转换为张量\n",
    "        done = torch.tensor([done], device=device)\n",
    "        \n",
    "        # agent.store(state, action, reward, next_state, done) # 存储经验\n",
    "        agent.store(state.cpu().numpy(), action, reward.cpu().numpy(), next_state.cpu().numpy(), done.cpu().numpy()) # 存储经验\n",
    "        state = next_state # 更新状态\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode: {}/{}, score: {}, cumulative reward: {}\".format(e, e_range, time, cumulative_reward)) # 打印每个episode的时间步数和累积奖励\n",
    "            break\n",
    "        if len(agent.memory) > batch_size: # 当记忆库中的样本数量大于32时, 开始经验回放\n",
    "            agent.replay(batch_size)\n",
    "    \n",
    "    # 每100个episode结束后, 保存模型的状态字典\n",
    "    if e % 100 == 0:\n",
    "        model_path = os.path.join(model_dir, f\"model_0.pth\")  # 使用 f-string 包含 episode 数\n",
    "        torch.save(agent.model.state_dict(), model_path)\n",
    "        print(\"模型状态字典已保存至\", model_path)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished, reward:  -137.61320100691955\n",
      "Episode finished, reward:  -146.1266235249094\n",
      "Episode finished, reward:  -134.11681625564947\n",
      "Episode finished, reward:  -119.53024549493182\n",
      "Episode finished, reward:  -121.0008451353155\n",
      "Episode finished, reward:  -119.51990182683784\n",
      "Episode finished, reward:  -208.85742491049533\n",
      "Episode finished, reward:  -103.61283370551436\n",
      "Episode finished, reward:  -123.82696442465235\n",
      "Episode finished, reward:  -140.45617900191112\n",
      "Episode finished, reward:  -123.11567327908492\n",
      "Episode finished, reward:  -209.07988022638415\n",
      "Episode finished, reward:  -104.04055850609487\n",
      "Episode finished, reward:  -130.1829931170504\n",
      "Episode finished, reward:  -133.65330156267362\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import torch# 确保从包含DQN类定义的文件中导入\n",
    "import os\n",
    "\n",
    "# 假设状态空间和动作空间的维度已知\n",
    "state_size = 8  # 根据你的环境设置\n",
    "action_size = 4  # 根据你的环境设置\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "model_dir = os.path.join(current_dir, \"models\")\n",
    "model_path = os.path.join(model_dir, \"model_0.pth\")\n",
    "\n",
    "model = DQN(state_size, action_size)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()  # 将模型设置为评估模式\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "\n",
    "cumulative_reward = 0  # 初始化累积奖励为0\n",
    "for _ in range(1000):\n",
    "    # 将观测转换为适合模型的格式\n",
    "    state = torch.tensor([observation], dtype=torch.float32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    with torch.no_grad():  # 禁止torch追踪此处的梯度计算，因为我们在推理而不是训练\n",
    "        action = model(state).max(1)[1].item()  # 获取最大Q值对应的动作\n",
    "\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    # print(\"reward: \", reward)\n",
    "    cumulative_reward += reward # 累积奖励\n",
    "    \n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(\"Episode finished, reward: \", cumulative_reward)\n",
    "        observation, info = env.reset()\n",
    "        cumulative_reward = 0\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-LUNARLANDER]",
   "language": "python",
   "name": "conda-env-.conda-LUNARLANDER-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
