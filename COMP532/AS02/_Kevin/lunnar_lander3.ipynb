{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (8,)\n",
      "State size:  8\n",
      "Number of actions:  4\n",
      "Episode 100\tAverage Score: -176.21\n",
      "Episode 200\tAverage Score: -90.492\n",
      "Episode 243\tAverage Score: -59.53"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 152\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(maximum_number_timesteps_per_episode):\n\u001b[1;32m    151\u001b[0m   action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state, epsilon)\n\u001b[0;32m--> 152\u001b[0m   next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m   loss \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mstep(state, action, reward, next_state, done)\n\u001b[1;32m    154\u001b[0m   state \u001b[38;5;241m=\u001b[39m next_state\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/gym/lib/python3.11/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/gym/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/gym/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/gym/lib/python3.11/site-packages/gymnasium/envs/box2d/lunar_lander.py:676\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m--> 676\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(state, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "  def __init__(self, state_size, action_size):\n",
    "    super(Network, self).__init__()\n",
    "    # self.seed = torch.manual_seed(seed)\n",
    "    self.fc1 = nn.Linear(state_size, 64)\n",
    "    self.fc2 = nn.Linear(64, 64)\n",
    "    self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "  def forward(self, state):\n",
    "    x = self.fc1(state)\n",
    "    x = F.relu(x)\n",
    "    x = self.fc2(x)\n",
    "    x = F.relu(x)\n",
    "    return self.fc3(x)\n",
    "  \n",
    "\n",
    "# Defining the hyperparameters\n",
    "\n",
    "learning_rate = 5e-4\n",
    "minibatch_size = 100\n",
    "discount_factor = 0.99\n",
    "replay_buffer_size = int(1e5)\n",
    "interpolation_parameter = 1e-3\n",
    "\n",
    "\n",
    "# Implementing the Experience Replay\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "  def __init__(self, capacity):\n",
    "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    self.capacity = capacity\n",
    "    self.memory = []\n",
    "\n",
    "  def push(self, event):\n",
    "    self.memory.append(event)\n",
    "    if len(self.memory) > self.capacity:\n",
    "      del self.memory[0]\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    experiences = random.sample(self.memory, k = batch_size)\n",
    "    states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(self.device)\n",
    "    actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(self.device)\n",
    "    rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(self.device)\n",
    "    next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(self.device)\n",
    "    dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "    return states, next_states, actions, rewards, dones\n",
    "\n",
    "\n",
    "class Agent():\n",
    "\n",
    "  def __init__(self, state_size, action_size):\n",
    "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    self.state_size = state_size\n",
    "    self.action_size = action_size\n",
    "    self.local_qnetwork = Network(state_size, action_size).to(self.device)\n",
    "    self.target_qnetwork = Network(state_size, action_size).to(self.device)\n",
    "    self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr = learning_rate)\n",
    "    self.memory = ReplayMemory(replay_buffer_size)\n",
    "    self.t_step = 0\n",
    "    self.loss = 0\n",
    "\n",
    "  def step(self, state, action, reward, next_state, done):\n",
    "    self.memory.push((state, action, reward, next_state, done))\n",
    "    self.t_step = (self.t_step + 1) % 4\n",
    "    if self.t_step == 0:\n",
    "      if len(self.memory.memory) > minibatch_size:\n",
    "        experiences = self.memory.sample(100)\n",
    "        self.learn(experiences, discount_factor)\n",
    "    \n",
    "\n",
    "  def act(self, state, epsilon = 0.):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "    self.local_qnetwork.eval()\n",
    "    with torch.no_grad():\n",
    "      action_values = self.local_qnetwork(state)\n",
    "    self.local_qnetwork.train()\n",
    "    if random.random() > epsilon:\n",
    "      return np.argmax(action_values.cpu().data.numpy())\n",
    "    else:\n",
    "      return random.choice(np.arange(self.action_size))\n",
    "\n",
    "  def learn(self, experiences, discount_factor):\n",
    "    states, next_states, actions, rewards, dones = experiences\n",
    "    next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "    q_targets = rewards + discount_factor * next_q_targets * (1 - dones)\n",
    "    q_expected = self.local_qnetwork(states).gather(1, actions)\n",
    "    loss = F.mse_loss(q_expected, q_targets)\n",
    "    self.loss = loss.item()\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "    self.soft_update(self.local_qnetwork, self.target_qnetwork, interpolation_parameter)\n",
    "\n",
    "  def soft_update(self, local_model, target_model, interpolation_parameter):\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "      target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data)\n",
    "\n",
    "\n",
    "  def get_loss(self):\n",
    "    return self.loss\n",
    "\n",
    "\n",
    "# Setting up the Lunar Environment\n",
    "\n",
    "import gymnasium as gym\n",
    "env = gym.make('LunarLander-v2')\n",
    "state_shape = env.observation_space.shape\n",
    "state_size = env.observation_space.shape[0]\n",
    "number_actions = env.action_space.n\n",
    "print('State shape: ', state_shape)\n",
    "print('State size: ', state_size)\n",
    "print('Number of actions: ', number_actions)\n",
    "\n",
    "### Initialize the Agent\n",
    "\n",
    "agent = Agent(state_size, number_actions)\n",
    "\n",
    "\n",
    "### Training the agent\n",
    "\n",
    "number_episodes = 2000\n",
    "maximum_number_timesteps_per_episode = 1000\n",
    "epsilon_starting_value  = 1.0\n",
    "epsilon_ending_value  = 0.01\n",
    "epsilon_decay_value  = 0.995\n",
    "epsilon = epsilon_starting_value\n",
    "scores_on_100_episodes = deque(maxlen = 100)\n",
    "\n",
    "\n",
    "with open(\"rewards.csv\", 'w') as f:\n",
    "  f.write(\"Episode,Reward,Loss,Done\\n\")\n",
    "for episode in range(1, number_episodes + 1):\n",
    "  state, _ = env.reset()\n",
    "  score = 0\n",
    "  loss = 0\n",
    "\n",
    "  for t in range(maximum_number_timesteps_per_episode):\n",
    "    action = agent.act(state, epsilon)\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    agent.step(state, action, reward, next_state, done)\n",
    "    loss = agent.get_loss()\n",
    "    state = next_state\n",
    "    score += reward\n",
    "    if done:\n",
    "      break\n",
    "      \n",
    "  with open(\"rewards.csv\", \"a\") as f:\n",
    "    f.write(str(episode) + ',' + str(score) + ',' + str(loss)+ ',' + str(done) + '\\n')\n",
    "  scores_on_100_episodes.append(score)\n",
    "  epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)\n",
    "  \n",
    "  print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end = \"\")\n",
    "  if episode % 100 == 0:\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))\n",
    "  if np.mean(scores_on_100_episodes) >= 250.0:\n",
    "    print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode - 100, np.mean(scores_on_100_episodes)))\n",
    "    torch.save(agent.local_qnetwork.state_dict(), 'checkpoint.pth')\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Loading the trained agent\n",
    "\n",
    "agent.local_qnetwork.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "# Testing the agent in the environment for lunar lander game\n",
    "def show_video_of_model(agent, env_name):\n",
    "    env = gym.make(env_name, render_mode='human')\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    frames = []\n",
    "    while not done:\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, _, _ = env.step(action.item())\n",
    "    env.close()\n",
    "    # imageio.mimsave('video2.mp4', frames, fps=30)\n",
    "show_video_of_model(agent, 'LunarLander-v2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not find a backend to open `video2.mp4`` with iomode `wI`.\nBased on the extension, the following plugins might add capable backends:\n  FFMPEG:  pip install imageio[ffmpeg]\n  pyav:  pip install imageio[pyav]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     19\u001b[0m     imageio\u001b[38;5;241m.\u001b[39mmimsave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo2.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m, frames, fps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mshow_video_of_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLunarLander-v2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow_video\u001b[39m():\n\u001b[1;32m     24\u001b[0m     mp4list \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m, in \u001b[0;36mshow_video_of_model\u001b[0;34m(agent, env_name)\u001b[0m\n\u001b[1;32m     17\u001b[0m     state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     18\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m---> 19\u001b[0m \u001b[43mimageio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmimsave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvideo2.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/gym/lib/python3.11/site-packages/imageio/v2.py:494\u001b[0m, in \u001b[0;36mmimwrite\u001b[0;34m(uri, ims, format, **kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m imopen_args \u001b[38;5;241m=\u001b[39m decypher_format_arg(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m    493\u001b[0m imopen_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 494\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mimopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mimopen_args\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mwrite(ims, is_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/gym/lib/python3.11/site-packages/imageio/core/imopen.py:281\u001b[0m, in \u001b[0;36mimopen\u001b[0;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         err_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    276\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBased on the extension, the following plugins might add capable backends:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_candidates\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         )\n\u001b[1;32m    280\u001b[0m request\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err_type(err_msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not find a backend to open `video2.mp4`` with iomode `wI`.\nBased on the extension, the following plugins might add capable backends:\n  FFMPEG:  pip install imageio[ffmpeg]\n  pyav:  pip install imageio[pyav]"
     ]
    }
   ],
   "source": [
    "\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import imageio\n",
    "from IPython.display import HTML, display\n",
    "# from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "def show_video_of_model(agent, env_name):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    frames = []\n",
    "    while not done:\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, _, _ = env.step(action.item())\n",
    "    env.close()\n",
    "    imageio.mimsave('video2.mp4', frames, fps=30)\n",
    "\n",
    "show_video_of_model(agent, 'LunarLander-v2')\n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display(HTML(data='''<video alt=\"test\" autoplay\n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video2/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "\n",
    "show_video()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LUNARLANDER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
