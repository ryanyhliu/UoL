{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Lsu1oL3hTGtu"
      },
      "source": [
        "## A simple game\n",
        "\n",
        "We need to define actions, states, state transitions, rewards and the discount factor.\n",
        "\n",
        "An MDP is a 5-tuple, $\\langle S, A, R, P, \\gamma \\rangle$\n",
        "\n",
        "--16 states\n",
        "\n",
        "--4 actions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r_bLH54S1_k",
        "outputId": "1d46d7c7-aa36-4397-e7eb-fc35877f1100"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 1. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 1.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 1.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 1.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 1.]\n",
            "  [0. 0. 0. ... 0. 0. 1.]\n",
            "  [0. 0. 0. ... 0. 0. 1.]\n",
            "  [0. 0. 0. ... 0. 0. 1.]]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the gridworld environment\n",
        "n_states = 16\n",
        "n_actions = 4\n",
        "P = np.zeros((n_states, n_actions, n_states))  # transition probabilities\n",
        "R = np.zeros((n_states, n_actions, n_states))  # rewards\n",
        "gamma = 0.9  # discount factor\n",
        "\n",
        "# Fill in the transition probabilities and rewards\n",
        "for s in range(n_states):\n",
        "    for a in range(n_actions):\n",
        "        if s == 0 or s == 15:\n",
        "            P[s, a, s] = 1\n",
        "        else:\n",
        "            if a == 0:  # up\n",
        "                s_prime = s - 4\n",
        "            elif a == 1:  # down\n",
        "                s_prime = s + 4\n",
        "            elif a == 2:  # left\n",
        "                s_prime = s - 1\n",
        "            else:  # right\n",
        "                s_prime = s + 1\n",
        "            if s_prime < 0:\n",
        "              s_prime = 0\n",
        "            if s_prime > 15:\n",
        "              s_prime = 15\n",
        "\n",
        "            if s_prime == 0:\n",
        "                R[s, a, s_prime] = -1  # start state\n",
        "            elif s_prime == 15:\n",
        "                R[s, a, s_prime] = 10  # goal state\n",
        "            else:\n",
        "                R[s, a, s_prime] = -1  # other states\n",
        "            P[s, a, s_prime] = 1\n",
        "\n",
        "print(P)\n",
        "# P[s, a, s_prime]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW6dD3sSVDCc"
      },
      "source": [
        "## Solution - DP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MbgBxKWUVCuY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]]\n",
            "(State  0 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  1 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  2 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  3 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  4 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  5 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  6 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  7 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  8 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  9 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  10 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  11 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  12 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  13 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  14 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  15 ) Actions [0.25 0.25 0.25 0.25]\n",
            "Value function:\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "Value function:\n",
            "[[ 0.         -2.36003667 -2.84272002 -2.80977835]\n",
            " [-2.8029887  -3.20188587 -3.02005022 -2.39774911]\n",
            " [-2.00161733 -1.60308234 -0.53564463  1.61922876]\n",
            " [ 2.35218862  3.05878273  5.06770607  0.        ]]\n"
          ]
        }
      ],
      "source": [
        "# Define the policy (arbitrary for now)\n",
        "policy = np.ones((n_states, n_actions)) / n_actions\n",
        "\n",
        "print(policy)\n",
        "for s in range(n_states):\n",
        "  print(\"(State \", s, \") Actions\", policy[s])\n",
        "\n",
        "# Policy evaluation algorithm\n",
        "V = np.zeros(n_states)  # initial value function estimate\n",
        "print(\"Value function:\")\n",
        "print(V.reshape(4, 4))\n",
        "tolerance = 1e-6  # convergence tolerance\n",
        "while True:\n",
        "    delta = 0\n",
        "    for s in range(n_states):\n",
        "        v = V[s]\n",
        "        bellman_update = 0\n",
        "        for a in range(n_actions):\n",
        "            for s_prime in range(n_states):\n",
        "                bellman_update += policy[s, a] * P[s, a, s_prime] * (R[s, a, s_prime] + gamma * V[s_prime])\n",
        "        V[s] = bellman_update\n",
        "        delta = max(delta, abs(v - V[s]))\n",
        "    if delta < tolerance:\n",
        "        break\n",
        "\n",
        "print(\"Value function:\")\n",
        "print(V.reshape(4, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY6O4iKMaj5a"
      },
      "source": [
        "## Q-learning algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzFly1mtafAd",
        "outputId": "5357890b-619d-4f20-86d7-56c35d7b2d71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal policy:\n",
            "[[0 1 3 1]\n",
            " [1 1 1 1]\n",
            " [1 1 3 1]\n",
            " [1 1 1 0]]\n"
          ]
        }
      ],
      "source": [
        "Q = np.zeros((n_states, n_actions))  # initial Q-values\n",
        "n_episodes = 10000  # number of episodes\n",
        "alpha = 0.1  # learning rate\n",
        "epsilon = 0.1  # epsilon-greedy exploration probability\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "    s = np.random.randint(n_states)  # randomly select initial state\n",
        "    while s not in [0, 15]:\n",
        "        # epsilon-greedy action selection\n",
        "        if np.random.uniform() < epsilon:\n",
        "            a = np.random.randint(n_actions)\n",
        "        else:\n",
        "            a = np.argmax(Q[s, :])\n",
        "        # take the selected action and observe the next state and reward\n",
        "        s_prime = np.random.choice(range(n_states), p=P[s, a, :])\n",
        "        r = R[s, a, s_prime]\n",
        "        # update Q-value for the current state-action pair\n",
        "        Q[s, a] = Q[s, a] + alpha * (r + gamma * np.max(Q[s_prime, :]) - Q[s, a])\n",
        "        s = s_prime\n",
        "\n",
        "# Extract the optimal policy from Q-values\n",
        "policy = np.argmax(Q, axis=1)\n",
        "\n",
        "print(\"Optimal policy:\")\n",
        "print(policy.reshape(4, 4))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
